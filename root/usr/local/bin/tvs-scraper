#!/usr/bin/env python3
"""
A lean web scraper for TVSpielfilm.de to extract TV program data.

This script uses the `requests` library for HTTP requests and `lxml.html`
with `cssselect` for efficient HTML parsing. It supports custom file-based
caching for HTTP responses and processed JSON data, parallel fetching
using a thread pool, and robust retry mechanisms for network and
application-level errors. It generates EPG data in XMLTV or JSON format.
"""
import argparse
import concurrent.futures
import functools
import json
import logging
import logging.handlers
import lxml.cssselect
import lxml.etree
import lxml.html
import os
import portalocker
import random
import re
import requests
import shutil
import sys
import tempfile
import threading
import time
from argparse import RawTextHelpFormatter
from datetime import datetime, timedelta, timezone
from requests.adapters import HTTPAdapter
from typing import List, Dict, Optional, Any, Tuple
from urllib.parse import urlparse, parse_qs
from urllib3.util.retry import Retry

if sys.version_info < (3, 9):
    print("Error: This script requires Python 3.9 or higher for the 'zoneinfo' module. Please update your Python version.")
    sys.exit(1)
from zoneinfo import ZoneInfo

logger = logging.getLogger(__name__)

DEFAULT_DAYS: int = 0
DEFAULT_IMG_SIZE: str = "600"
DEFAULT_IMG_CHECK: bool = False
MAX_DAYS_TO_SCRAPE: int = 13

DEFAULT_CACHE_SUBDIR: str = "tvs-cache"
DEFAULT_CACHE_DISABLE: bool = False
DEFAULT_CACHE_TTL_SECONDS: int = 6 * 60 * 60
CACHE_PROCESSED_DATA_SUBDIR: str = "processed-data"
CACHE_CHANNEL_LIST_FILE: str = "channels.json"

DEFAULT_MAX_WORKERS: int = 10
DEFAULT_MAX_RETRIES: int = 5
RETRY_STATUS_FORCELIST: List[int] = [429, 500, 502, 503, 504]
RETRY_BACKOFF_FACTOR: float = 1.0
RETRY_ON_CONNECTION_ERRORS: bool = True

DEFAULT_OUTPUT_FORMAT: str = 'xmltv'
DEFAULT_SYSLOG_TAG: str = 'tvs-scraper'
DEFAULT_MIN_REQUEST_DELAY: float = 0.05
DEFAULT_MAX_SCHEDULE_RETRIES: int = 3
DEFAULT_CACHE_VALIDATION_TOLERANCE: int = 5
DEFAULT_HTTP_TIMEOUT: int = 10

USER_AGENTS: List[str] = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:138.0) Gecko/20100101 Firefox/138.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/605.1.15',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36 Edg/136.0.0.0',
    'Mozilla/5.0 (X11; Linux x86_64; rv:138.0) Gecko/20100101 Firefox/138.0',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36',
]

BASE_URL: str = "https://m.tvspielfilm.de"
CHANNEL_LIST_URL: str = f"{BASE_URL}/sender/"

re_cast_entry_match = re.compile(r'(.+?)\s*\((.+?)\)')
re_channel_ids_pattern = re.compile(r'^[A-Z0-9,_-]+$')

_INVALID_XML_CHARS_PATTERN = re.compile(
    '[\x00-\x08\x0B\x0C\x0E-\x1F\x7F-\x9F\uFDD0-\uFDEF\uFFFE\uFFFF]'
)


def _sanitize_xml_string(text: Optional[str]) -> Optional[str]:
    """
    Removes characters from a string that are invalid in XML 1.0 documents.

    :param text: The input string.
    :type text: Optional[str]
    :returns: The sanitized string, or None if input was None.
    :rtype: Optional[str]
    """
    if text is None:
        return None
    text = str(text)
    return _INVALID_XML_CHARS_PATTERN.sub('', text)


def _convert_unix_to_xmltv_time(unix_timestamp_str: Optional[str], target_timezone_str: str = 'UTC') -> Optional[str]:
    """
    Converts a Unix timestamp string (in seconds) to XMLTV time format (YYYYMMDDhhmmss +ZZZZ).

    :param unix_timestamp_str: Unix timestamp as a string.
    :type unix_timestamp_str: Optional[str]
    :param target_timezone_str: The target timezone string (e.g., 'Europe/Berlin').
    :type target_timezone_str: str
    :returns: XMLTV formatted time string, or None if conversion fails.
    :rtype: Optional[str]
    """
    if not unix_timestamp_str:
        return None
    try:
        unix_timestamp: int = int(unix_timestamp_str)

        if unix_timestamp < 946684800:  # Sat Jan 01 2000 00:00:00 GMT+0000
            logger.warning(f"Unrealistically old Unix timestamp '{unix_timestamp_str}' for XMLTV conversion received. Skipping.")
            return None

        dt_object_utc: datetime = datetime.fromtimestamp(unix_timestamp, tz=timezone.utc)

        try:
            target_tz = ZoneInfo(target_timezone_str)
        except Exception as e:
            logger.error(f"Unknown timezone '{target_timezone_str}'. Falling back to UTC. Error: {e}")
            target_tz = timezone.utc

        dt_object_local: datetime = dt_object_utc.astimezone(target_tz)

        offset_seconds = dt_object_local.utcoffset().total_seconds()
        offset_hours = int(offset_seconds // 3600)
        offset_minutes = int((abs(offset_seconds) % 3600) // 60)
        offset_sign = '+' if offset_hours >= 0 else '-'
        offset_str = f"{offset_sign}{abs(offset_hours):02d}{abs(offset_minutes):02d}"

        return dt_object_local.strftime(f'%Y%m%d%H%M%S {offset_str}')
    except (ValueError, TypeError) as e:
        logger.warning(f"Could not convert Unix timestamp '{unix_timestamp_str}' to XMLTV time: {e}")
        return None


def _process_rating_string(original_rating_string: Optional[str]) -> Optional[str]:
    """
    Cleans and formats the rating string.

    :param original_rating_string: The raw rating string.
    :type original_rating_string: Optional[str]
    :returns: The cleaned and formatted rating string, or None if input was None.
    :rtype: Optional[str]
    """
    if not original_rating_string:
        return None
    parts: List[str] = [p.strip() for p in original_rating_string.split('/')]
    filtered_parts: List[str] = [part for part in parts if '*' in part]
    return " / ".join(filtered_parts) if filtered_parts else None


class TvsHtmlParser:
    """
    Encapsulates all HTML parsing logic for TVSpielfilm.de.
    """
    def __init__(self):
        """
        Initializes the TvsHtmlParser with all necessary CSS selectors
        and regular expressions for parsing.
        """
        self.re_source_id_html: re.Pattern = re.compile(r',([a-zA-Z0-9_-]+)\.html$')
        self.re_image_url_match: re.Pattern = re.compile(r"url\(['\"]?(.*?)['\"]?\)")
        self.re_image_base_path_match: re.Pattern = re.compile(r'(.*?)_(\d+)\.jpg$')
        self.re_log_item_id_cleanup: re.Pattern = re.compile(r'[^a-zA-Z0-9]')
        self.re_genre_pipe_match: re.Pattern = re.compile(r'\|\s*(.*)')
        self.re_duration_minutes_match: re.Pattern = re.compile(r'(\d+)\s*Min\.', re.IGNORECASE)
        self.re_fsk_match: re.compile = re.compile(r'(\d+)')
        self.re_rating_class_match: re.Pattern = re.compile(r'rating-(\d+)')
        self.re_adsc_sparte_match: re.compile = re.compile(r'"adsc_sparte":"([^"]+)"')

        self.selector_sender_links: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('div.component.channels.all-channels.abc-scroll ul li a')
        self.selector_li_elements: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('li.tv-tip.time-listing.js-tv-show.channels')
        self.selector_title_tag: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('strong.tv-tip-heading span.title')
        self.selector_detail_link_tag: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('div.image-text-holder a.flex-row.js-track-link')
        self.selector_image_div: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('div.program-image div.default-image')

        self.base_detail_list_container: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('div.detail-tab__container-content[data-detail-tab-index="1"] div.list-container div.definition-list')
        self.selector_dl: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('dl')
        self.selector_dt: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('dt')
        self.selector_dd: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('dd')
        self.selector_headline_p: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('p.headline')

        self.selector_description_section: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('section.broadcast-detail__description')
        self.selector_short_desc_tag: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('p.headline')
        self.selector_long_desc_paragraphs: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('p:not(.headline)')
        self.selector_genre_underline_tag: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('div.stage-underline.gray span.text-row')
        self.selector_genre_tag_general: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('div.genre-info span.genre-text')
        self.selector_genre_time_tag_schedule: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('div.genre span.genre-time')
        self.selector_content_rating_section: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('section.content-rating')
        self.selector_num_rating_div: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('div.content-rating__rating-genre__thumb')
        self.selector_txt_rating_tag: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('blockquote.content-rating__rating-genre__conclusion-quote p')
        self.selector_rating_list_items: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('ul.content-rating__rating-genre__list li.content-rating__rating-genre__list-item')
        self.selector_rating_label: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('span.content-rating__rating-genre__list-item__label')
        self.selector_rating_span: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('span.content-rating__rating-genre__list-item__rating')
        self.selector_imdb_rating_tag: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('div.content-rating__imdb-rating__rating-value')

    def parse_channel_list(self, html_content: str) -> List[Dict[str, str]]:
        """
        Parses the list of channels from the HTML content of the main sender page.

        :param html_content: The HTML content of the sender page.
        :type html_content: str
        :returns: A list of dictionaries, each containing 'name', 'url', and 'source_id' for a channel.
        :rtype: List[Dict[str, str]]
        """
        root: lxml.html.HtmlElement = lxml.html.fromstring(html_content)
        sender_links: List[lxml.html.HtmlElement] = self.selector_sender_links(root)
        logger.debug(f"Found {len(sender_links)} potential channel links.")

        channel_data: List[Dict[str, str]] = []
        for link in sender_links:
            href: Optional[str] = link.get('href')
            name: Optional[str] = None
            if link.text_content():
                name = _sanitize_xml_string(link.text_content().strip())
            else:
                logger.warning(f"Channel link found with no text content for href: {href}. Skipping.")
                continue

            source_id: Optional[str] = None
            if href:
                source_id_match: Optional[re.Match[str]] = self.re_source_id_html.search(href)
                if source_id_match:
                    source_id = source_id_match.group(1).upper()

            if not source_id:
                tracking_data_str: Optional[str] = link.get('data-tracking-point')
                if tracking_data_str:
                    try:
                        tracking_data: Dict[str, Any] = json.loads(tracking_data_str)
                        if 'channel' in tracking_data:
                            source_id = str(tracking_data['channel']).upper()
                    except json.JSONDecodeError:
                        logger.debug(f"Could not parse data-tracking-point for {name}: {tracking_data_str}")

            if source_id and name:
                logger.debug(f"Processing channel: {name} (ID: {source_id}, Href: {href})")
                channel_data.append({
                    'name': name,
                    'url': f"{BASE_URL}{href}" if href and href.startswith('/') else (href if href else ''),
                    'source_id': source_id
                })
            else:
                logger.warning(f"Could not extract source_id for channel: {name} (Href: {href}). Skipping.")
        return channel_data

    def parse_daily_schedule_items(self, html_content: str, channel_info: Dict[str, str], current_date: datetime.date) -> List[Dict[str, Any]]:
        """
        Parses the schedule for a single channel and a specific date from the HTML content.

        This method extracts basic program information and detail links/image paths.

        :param html_content: The HTML content of the schedule page.
        :type html_content: str
        :param channel_info: Dictionary containing channel information ('name', 'url', 'source_id').
        :type channel_info: Dict[str, str]
        :param current_date: The date for which the schedule is being parsed.
        :type current_date: datetime.date
        :returns: A list of dictionaries, each representing a program item,
                  with basic information and full image URLs.
        :rtype: List[Dict[str, Any]]
        """
        channel_name: str = channel_info['name']
        channel_source_id: str = channel_info['source_id']

        root: lxml.html.HtmlElement = lxml.html.fromstring(html_content)

        extracted_items: List[Dict[str, Any]] = []

        for li_element in self.selector_li_elements(root):
            item_data: Dict[str, Any] = {}

            item_data['type'] = 'program'
            item_data['channelname'] = channel_name
            item_data['sourceid'] = channel_source_id
            item_data['date'] = current_date.strftime('%Y-%m-%d')

            data_start_time = li_element.get('data-start-time')
            if data_start_time:
                item_data['starttime'] = data_start_time
            else:
                logger.warning(f"Missing 'data-start-time' for an item in {channel_name}'s schedule on {current_date.strftime('%Y-%m-%d')}. Skipping item.")
                continue

            data_end_time = li_element.get('data-end-time')
            if data_end_time:
                item_data['endtime'] = data_end_time
            else:
                logger.warning(f"Missing 'data-end-time' for item starting at {item_data['starttime']} in {channel_name}'s schedule on {current_date.strftime('%Y-%m-%d')}.")
                item_data['endtime'] = None

            title_tag: List[lxml.html.HtmlElement] = self.selector_title_tag(li_element)
            if title_tag:
                item_data['title'] = _sanitize_xml_string(title_tag[0].text_content().strip())
            else:
                logger.error(f"Missing title for item starting at {item_data['starttime']} in {channel_name}'s schedule on {current_date.strftime('%Y-%m-%d')}. Skipping entire item due to missing title.")
                continue

            detail_link_tag: List[lxml.html.HtmlElement] = self.selector_detail_link_tag(li_element)
            if detail_link_tag:
                detail_href: Optional[str] = detail_link_tag[0].get('href')
                if detail_href:
                    if not detail_href.startswith('http'):
                        item_data['link'] = f"{BASE_URL}{detail_href}"
                    else:
                        item_data['link'] = detail_href
                else:
                    logger.debug(f"Detail link found but no href for item '{item_data['title']}' in {channel_name}'s schedule on {current_date.strftime('%Y-%m-%d')}.")
                    item_data['link'] = None
            else:
                logger.debug(f"Missing detail link tag for item '{item_data['title']}' in {channel_name}'s schedule on {current_date.strftime('%Y-%m-%d')}.")
                item_data['link'] = None

            data_id = li_element.get('data-id')
            if data_id:
                item_data['eventid'] = data_id
            else:
                logger.debug(f"Missing 'data-id' for item '{item_data['title']}' in {channel_name}'s schedule on {current_date.strftime('%Y-%m-%d')}.")
                item_data['eventid'] = None

            image_div: List[lxml.html.HtmlElement] = self.selector_image_div(li_element)
            if image_div:
                style_attr: Optional[str] = image_div[0].get('style')
                if style_attr:
                    image_url_match: Optional[re.Match[str]] = self.re_image_url_match.search(style_attr)
                    if image_url_match:
                        item_data['image_url'] = image_url_match.group(1)
                    else:
                        logger.debug(f"No image URL found in style attribute for item '{item_data['title']}' in {channel_name}'s schedule on {current_date.strftime('%Y-%m-%d')}.")
                        item_data['image_url'] = None
                else:
                    logger.debug(f"Missing style attribute for image div for item '{item_data['title']}' in {channel_name}'s schedule on {current_date.strftime('%Y-%m-%d')}.")
                    item_data['image_url'] = None
            else:
                logger.debug(f"Missing image div for item '{item_data['title']}' in {channel_name}'s schedule on {current_date.strftime('%Y-%m-%d')}.")
                item_data['image_url'] = None

            extracted_items.append(item_data)
        return extracted_items

    def _extract_description(self, root: lxml.html.HtmlElement, detail_data: Dict[str, Any]) -> None:
        """
        Extracts short and long descriptions from the HTML root.

        :param root: The lxml HTML root element.
        :type root: lxml.html.HtmlElement
        :param detail_data: The dictionary to update with extracted data.
        :type detail_data: Dict[str, Any]
        """
        description_section: List[lxml.html.HtmlElement] = self.selector_description_section(root)
        if description_section:
            short_desc_tag: List[lxml.html.HtmlElement] = self.selector_short_desc_tag(description_section[0])
            if short_desc_tag and short_desc_tag[0].text_content():
                detail_data['shortdescription'] = _sanitize_xml_string(short_desc_tag[0].text_content().strip())
            else:
                logger.debug("Short description tag not found or empty.")

            long_desc_paragraphs: List[lxml.html.HtmlElement] = self.selector_long_desc_paragraphs(description_section[0])
            if long_desc_paragraphs:
                cleaned_paragraphs = [p.text_content().strip() for p in long_desc_paragraphs if p.text_content().strip()]
                if cleaned_paragraphs:
                    detail_data['longdescription'] = _sanitize_xml_string("\n".join(cleaned_paragraphs))
            else:
                logger.debug("Long description paragraphs not found or empty.")
        else:
            logger.debug("Description section not found.")

    def _extract_genre_info(self, root: lxml.html.HtmlElement, detail_data: Dict[str, Any]) -> None:
        """
        Extracts genre information from the HTML root.

        :param root: The lxml HTML root element.
        :type root: lxml.html.HtmlElement
        :param detail_data: The dictionary to update with extracted data.
        :type detail_data: Dict[str, Any]
        """
        genre_underline_tag: List[lxml.html.HtmlElement] = self.selector_genre_underline_tag(root)
        if genre_underline_tag and genre_underline_tag[0].text_content():
            genre_text: str = genre_underline_tag[0].text_content().strip()
            genre_match: Optional[re.Match[str]] = self.re_genre_pipe_match.search(genre_text)
            if genre_match:
                detail_data['genre'] = _sanitize_xml_string(genre_match.group(1).strip())
            else:
                detail_data['genre'] = _sanitize_xml_string(genre_text.strip())
        else:
            logger.debug("Genre underline tag not found or empty.")

        if not detail_data['genre']:
            for infos_container_elem in self.base_detail_list_container(root):
                headline_p_elements: List[lxml.html.HtmlElement] = self.selector_headline_p(infos_container_elem)
                if headline_p_elements and headline_p_elements[0].text_content().strip() == "Infos":
                    for dt_element in self.selector_dt(infos_container_elem):
                        if dt_element.text_content().strip() == "Genre":
                            genre_dd: Optional[lxml.html.HtmlElement] = dt_element.getnext()
                            if genre_dd and genre_dd.text_content():
                                detail_data['genre'] = _sanitize_xml_string(genre_dd.text_content().strip())
                                break
                            else:
                                logger.debug("Genre DD tag not found or empty after 'Genre' DT tag.")
                    if detail_data['genre']:
                        break
                else:
                    logger.debug("Information headline not found for genre extraction.")

        if not detail_data['genre']:
            genre_tag_general: List[lxml.html.HtmlElement] = self.selector_genre_tag_general(root)
            if genre_tag_general and genre_tag_general[0].text_content():
                detail_data['genre'] = _sanitize_xml_string(genre_tag_general[0].text_content().strip())
            else:
                logger.debug("General genre tag not found or empty.")
                genre_time_tag_schedule: List[lxml.html.HtmlElement] = self.selector_genre_time_tag_schedule(root)
                if genre_time_tag_schedule and genre_time_tag_schedule[0].text_content():
                    genre_match = self.re_genre_pipe_match.search(genre_time_tag_schedule[0].text_content().strip())
                    if genre_match:
                        detail_data['genre'] = _sanitize_xml_string(genre_match.group(1).strip())
                    else:
                        detail_data['genre'] = _sanitize_xml_string(genre_time_tag_schedule[0].text_content().strip())
                else:
                    logger.debug("Schedule genre time tag not found or empty.")

    def _extract_general_info(self, root: lxml.html.HtmlElement, detail_data: Dict[str, Any]) -> None:
        """
        Extracts general information (original title, country, year, duration, parental rating)
        from the HTML root.

        :param root: The lxml HTML root element.
        :type root: lxml.html.HtmlElement
        :param detail_data: The dictionary to update with extracted data.
        :type detail_data: Dict[str, Any]
        """
        for infos_container_elem in self.base_detail_list_container(root):
            headline_p_elements = self.selector_headline_p(infos_container_elem)
            if headline_p_elements and headline_p_elements[0].text_content().strip() == "Infos":
                infos_dl: List[lxml.html.HtmlElement] = self.selector_dl(infos_container_elem)
                if infos_dl:
                    dt_elements: List[lxml.html.HtmlElement] = self.selector_dt(infos_dl[0])
                    dd_elements: List[lxml.html.HtmlElement] = self.selector_dd(infos_dl[0])
                    for dt, dd in zip(dt_elements, dd_elements):
                        dt_text: str = dt.text_content().strip()
                        dd_text: str = dd.text_content().strip()
                        if dt_text == 'Originaltitel':
                            detail_data['original_title'] = _sanitize_xml_string(dd_text)
                        elif dt_text == 'Land':
                            detail_data['country'] = _sanitize_xml_string(dd_text)
                        elif dt_text == 'Jahr':
                            try:
                                detail_data['year'] = int(dd_text)
                            except ValueError:
                                logger.warning(f"Could not parse year from '{dd_text}'.")
                        elif dt_text == 'LÃ¤nge':
                            duration_minutes_match: Optional[re.Match[str]] = self.re_duration_minutes_match.search(dd_text)
                            if duration_minutes_match:
                                detail_data['duration'] = int(duration_minutes_match.group(1)) * 60
                            else:
                                logger.warning(f"Could not parse duration from '{dd_text}'.")
                        elif dt_text == 'FSK':
                            fsk_match: Optional[re.Match[str]] = self.re_fsk_match.search(dd_text)
                            if fsk_match:
                                try:
                                    detail_data['parentalrating'] = int(fsk_match.group(1))
                                except ValueError:
                                    logger.warning(f"Could not parse FSK rating from '{fsk_match.group(1)}'.")
                else:
                    logger.debug("Information DL tag not found.")
                break
            else:
                logger.debug("Information headline not found for general info extraction.")

    def _extract_cast_info(self, root: lxml.html.HtmlElement, detail_data: Dict[str, Any]) -> None:
        """
        Extracts cast information from the HTML root.

        :param root: The lxml HTML root element.
        :type root: lxml.html.HtmlElement
        :param detail_data: The dictionary to update with extracted data.
        :type detail_data: Dict[str, Any]
        """
        for cast_container_elem in self.base_detail_list_container(root):
            headline_p_elements = self.selector_headline_p(cast_container_elem)
            if headline_p_elements and headline_p_elements[0].text_content().strip() == "Cast":
                cast_list: List[str] = []
                cast_dl: List[lxml.html.HtmlElement] = self.selector_dl(cast_container_elem)
                if cast_dl:
                    for dt_tag in self.selector_dt(cast_dl[0]):
                        role: str = dt_tag.text_content().strip()
                        actor_dd_tag: Optional[lxml.html.HtmlElement] = dt_tag.getnext()
                        if actor_dd_tag is not None and actor_dd_tag.text_content():
                            actor_name: str = actor_dd_tag.text_content().strip()
                            if actor_name:
                                cast_list.append(_sanitize_xml_string(f"{actor_name} ({role})" if role else actor_name) or "")
                        else:
                            logger.debug(f"Actor DD tag not found or empty for role '{role}'.")
                else:
                    logger.debug("Cast DL tag not found.")
                detail_data['cast'] = ", ".join(cast_list) if cast_list else None
                break
            else:
                logger.debug("Cast headline not found for cast info extraction.")

    def _extract_crew_info(self, root: lxml.html.HtmlElement, detail_data: Dict[str, Any]) -> None:
        """
        Extracts crew information (director, screenplay, camera) from the HTML root.

        :param root: The lxml HTML root element.
        :type root: lxml.html.HtmlElement
        :param detail_data: The dictionary to update with extracted data.
        :type detail_data: Dict[str, Any]
        """
        for crew_container_elem in self.base_detail_list_container(root):
            headline_p_elements = self.selector_headline_p(crew_container_elem)
            if headline_p_elements and headline_p_elements[0].text_content().strip() == "Crew":
                crew_dl: List[lxml.html.HtmlElement] = self.selector_dl(crew_container_elem)
                if crew_dl:
                    dt_elements = self.selector_dt(crew_dl[0])
                    dd_elements = self.selector_dd(crew_dl[0])
                    for dt, dd in zip(dt_elements, dd_elements):
                        dt_text: str = dt.text_content().strip()
                        dd_text: str = dd.text_content().strip()
                        if dd_text:
                            if dt_text == 'Regie':
                                detail_data['director'] = _sanitize_xml_string(dd_text)
                            elif dt_text == 'Drehbuch':
                                sanitized_dd_text: Optional[str] = _sanitize_xml_string(dd_text)
                                detail_data['screenplay'] = f"{detail_data['screenplay']}, {sanitized_dd_text}" if detail_data['screenplay'] else sanitized_dd_text
                            elif dt_text == 'Kamera':
                                detail_data['camera'] = _sanitize_xml_string(dd_text)
                        else:
                            logger.debug(f"Empty DD tag found for DT: '{dt_text}'.")
                else:
                    logger.debug("Crew DL tag not found.")
                break
            else:
                logger.debug("Crew headline not found for crew info extraction.")


    def _extract_rating_info(self, root: lxml.html.HtmlElement, detail_data: Dict[str, Any]) -> None:
        """
        Extracts rating information (numerical, text, detailed, IMDB) from the HTML root.

        :param root: The lxml HTML root element.
        :type root: lxml.html.HtmlElement
        :param detail_data: The dictionary to update with extracted data.
        :type detail_data: Dict[str, Any]
        """
        content_rating_section: List[lxml.html.HtmlElement] = self.selector_content_rating_section(root)
        if content_rating_section:
            num_rating_div: List[lxml.html.HtmlElement] = self.selector_num_rating_div(content_rating_section[0])
            if num_rating_div and num_rating_div[0].get('class'):
                class_attr: List[str] = num_rating_div[0].get('class', '').split()
                rating_class_match: Optional[re.Match[str]] = self.re_rating_class_match.search(' '.join(class_attr))
                if rating_class_match:
                    try:
                        detail_data['numrating'] = int(rating_class_match.group(1))
                    except ValueError:
                        logger.warning(f"Could not convert numerical rating class '{rating_class_match.group(1)}' to integer.")
                else:
                    logger.debug("Rating class match not found for numerical rating div.")
            else:
                logger.debug("Numerical rating div not found or has no class attribute.")

            txt_rating_tag: List[lxml.html.HtmlElement] = self.selector_txt_rating_tag(content_rating_section[0])
            if txt_rating_tag and txt_rating_tag[0].text_content():
                detail_data['txtrating'] = _sanitize_xml_string(txt_rating_tag[0].text_content().strip())
            else:
                logger.debug("Text rating tag not found or empty.")

            rating_list_items: List[lxml.html.HtmlElement] = self.selector_rating_list_items(content_rating_section[0])
            if rating_list_items:
                ratings_parts: List[str] = []
                for item in rating_list_items:
                    label: List[lxml.html.HtmlElement] = self.selector_rating_label(item)
                    rating_span: List[lxml.html.HtmlElement] = self.selector_rating_span(item)
                    if label and label[0].text_content() and rating_span and rating_span[0].get('class'):
                        rating_class_match = self.re_rating_class_match.search(' '.join(rating_span[0].get('class', '').split()))
                        if rating_class_match:
                            num_dots: int = int(rating_class_match.group(1))
                            stars: str = '*' * num_dots
                            ratings_parts.append(f"{label[0].text_content().strip()} {stars}")
                        else:
                            logger.debug("Rating class match not found for rating list item span.")
                    else:
                        logger.debug("Label or rating span not found/empty for a rating list item.")
                if ratings_parts:
                    detail_data['rating'] = _sanitize_xml_string(" / ".join(ratings_parts))
            else:
                logger.debug("Rating list items not found.")

            imdb_rating_tag: List[lxml.html.HtmlElement] = self.selector_imdb_rating_tag(content_rating_section[0])
            if imdb_rating_tag and imdb_rating_tag[0].text_content():
                detail_data['imdbrating'] = _sanitize_xml_string(imdb_rating_tag[0].text_content().strip())
            else:
                logger.debug("IMDB rating tag not found or empty.")
        else:
            logger.debug("Content rating section not found.")

    def parse_program_details(self, html_content: str, detail_data_template: Dict[str, Any]) -> Dict[str, Any]:
        """
        Parses the detail page of a program to extract additional information.

        :param html_content: The HTML content of the program detail page.
        :type html_content: str
        :param detail_data_template: A template dictionary for initializing detail data.
        :type detail_data_template: Dict[str, Any]
        :returns: A dictionary containing detailed program information.
        :rtype: Dict[str, Any]
        """
        detail_data: Dict[str, Any] = detail_data_template.copy()
        root: lxml.html.HtmlElement = lxml.html.fromstring(html_content)

        adsc_sparte_match: Optional[re.Match[str]] = self.re_adsc_sparte_match.search(html_content)
        if adsc_sparte_match:
            detail_data['keyword'] = _sanitize_xml_string(adsc_sparte_match.group(1).strip())

        self._extract_description(root, detail_data)
        self._extract_genre_info(root, detail_data)
        self._extract_general_info(root, detail_data)
        self._extract_cast_info(root, detail_data)
        self._extract_crew_info(root, detail_data)
        self._extract_rating_info(root, detail_data)

        return detail_data


class TvsLeanScraper:
    """
    Encapsulates the scraping logic for TVSpielfilm.de.

    Handles fetching HTML, parsing channel lists and program schedules,
    extracting detailed program information, and managing caching and retries.
    """
    _DETAIL_DATA_TEMPLATE: Dict[str, Any] = {
        'shortdescription': None, 'longdescription': None, 'genre': None,
        'original_title': None, 'country': None, 'year': None, 'duration': None,
        'parentalrating': None, 'cast': None, 'director': None, 'screenplay': None,
        'camera': None, 'numrating': None, 'rating': None, 'txtrating': None,
        'imdbrating': None, 'keyword': None
    }

    def __init__(self, channel_ids: Optional[str] = None, days: Optional[int] = None, img_size: Optional[str] = None, check_img: Optional[bool] = None, start_date_str: Optional[str] = None,
                 cache_dir_path: Optional[str] = None, max_workers: int = DEFAULT_MAX_WORKERS, max_retries: int = DEFAULT_MAX_RETRIES,
                 channel_ids_file: Optional[str] = None, min_request_delay: float = DEFAULT_MIN_REQUEST_DELAY,
                 max_schedule_retries: int = DEFAULT_MAX_SCHEDULE_RETRIES,
                 disable_cache: bool = DEFAULT_CACHE_DISABLE, cache_ttl: int = DEFAULT_CACHE_TTL_SECONDS,
                 keep_past_cache: bool = False, cache_clear: bool = False, xmltv_timezone: Optional[str] = None,
                 cache_validation_tolerance: int = DEFAULT_CACHE_VALIDATION_TOLERANCE, img_crop_disable: bool = False,
                 http_timeout: int = DEFAULT_HTTP_TIMEOUT):
        """
        Initializes the TvsLeanScraper instance.

        :param channel_ids: Comma-separated string of channel IDs to scrape.
        :type channel_ids: Optional[str]
        :param days: Number of days to scrape.
        :type days: Optional[int]
        :param img_size: Desired image size ("150", "300" or "600").
        :type img_size: Optional[str]
        :param check_img: Boolean to enable/disable image URL validity checks.
        :type check_img: Optional[bool]
        :param start_date_str: Specific date to scrape inYYYYMMDD format.
        :type start_date_str: Optional[str]
        :param cache_dir_path: Custom directory for cache files.
        :type cache_dir_path: Optional[str]
        :param max_workers: Maximum number of concurrent workers.
        :type max_workers: int
        :param max_retries: Maximum number of retries for failed HTTP requests.
        :type max_retries: int
        :param channel_ids_file: Path to a file containing comma-separated channel IDs.
        :type channel_ids_file: Optional[str]
        :param min_request_delay: Minimum delay in seconds between HTTP requests.
        :type min_request_delay: float
        :param max_schedule_retries: Maximum retries for application-level schedule parsing errors.
        :type max_schedule_retries: int
        :param disable_cache: If True, disables processed JSON data caching.
        :type disable_cache: bool
        :param cache_ttl: Cache Time To Live in seconds.
        :type cache_ttl: int
        :param keep_past_cache: If True, prevents automatic deletion of past days' cache files.
        :type keep_past_cache: bool
        :param cache_clear: If True, clears all caches before starting.
        :type cache_clear: bool
        :param xmltv_timezone: Specifies the timezone for XMLTV output.
        :type xmltv_timezone: Optional[str]
        :param cache_validation_tolerance: Tolerance in bytes for content-length comparison when ETag/Last-Modified fails to return 304.
        :type cache_validation_tolerance: int
        :param img_crop_disable: If True, disables default image cropping.
        :type img_crop_disable: bool
        :param http_timeout: Timeout for HTTP requests in seconds.
        :type http_timeout: int
        :raises NotADirectoryError: If the specified cache path exists but is not a directory.
        :raises SystemExit: If an invalid argument format is provided for `--date` or `--channel-ids`.
        """
        self.target_sourceids: Optional[set[str]] = None
        if channel_ids_file:
            try:
                with open(channel_ids_file, 'r', encoding='utf-8') as f:
                    file_content: str = f.read().strip()
                if file_content:
                    if not re_channel_ids_pattern.match(file_content):
                        parser.error(f"Invalid characters in channel IDs file '{channel_ids_file}'. Only A-Z, 0-9, ',', '-', '_' are allowed.")
                    self.target_sourceids = {cid.strip().upper() for cid in file_content.split(',') if cid.strip()}
                    logger.info(f"Target channels from file '{channel_ids_file}': {self.target_sourceids}")
                else:
                    logger.warning(f"Channel IDs file '{channel_ids_file}' is empty. All found channels will be crawled.")
            except FileNotFoundError:
                logger.error(f"Channel IDs file '{channel_ids_file}' not found. All found channels will be crawled.")
            except Exception as e:
                logger.error(f"Error reading channel IDs from file '{channel_ids_file}': {e}. All found channels will be crawled.")
        elif channel_ids:
            if isinstance(channel_ids, str):
                if not re_channel_ids_pattern.match(channel_ids):
                    parser.error("Invalid characters in --channel-ids argument. Only A-Z, 0-9, ',', '-', '_' are allowed.")
                self.target_sourceids = {cid.strip().upper() for cid in channel_ids.split(',') if cid.strip()}
                logger.info(f"Target channels from argument: {self.target_sourceids}")
            else:
                logger.warning(f"Invalid ID type for channel_ids: {type(channel_ids)}. Expected string. Crawling all found channels.")
        else:
            logger.debug("No specific channels targeted. Crawling all found channels.")

        self.is_specific_date_set: bool = False
        self.start_date: datetime.date = datetime.now().date()

        if start_date_str:
            self.is_specific_date_set = True
            try:
                self.start_date = datetime.strptime(start_date_str, '%Y%m%d').date()
                self.days_to_scrape: int = 1
            except ValueError:
                parser.error(f"Invalid DATE format for '--date': '{start_date_str}'. Expected WHENMMDD (e.g., 20250523).")
        else:
            self.days_to_scrape = int(days) if days is not None else DEFAULT_DAYS

        today = datetime.now().date()
        earliest_allowed_date = today - timedelta(days=1)
        latest_allowed_date = today + timedelta(days=MAX_DAYS_TO_SCRAPE - 1)

        if self.is_specific_date_set:
            if self.start_date < earliest_allowed_date:
                parser.error(f"Error: The specified date '{self.start_date.strftime('%Y%m%d')}' is too far in the past. Only dates from '{earliest_allowed_date.strftime('%Y%m%d')}' (yesterday) are allowed.")
            if self.start_date > latest_allowed_date:
                parser.error(f"Error: The specified date '{self.start_date.strftime('%Y%m%d')}' is too far in the future. Only dates up to '{latest_allowed_date.strftime('%Y%m%d')}' are allowed (maximum {MAX_DAYS_TO_SCRAPE} days from today).")

        if self.days_to_scrape < 1:
            logger.info(f"DAYS value {self.days_to_scrape} is less than 1. Setting to 1 day (today).")
            self.days_to_scrape = 1
        elif self.days_to_scrape > MAX_DAYS_TO_SCRAPE:
            logger.warning(f"DAYS value {self.days_to_scrape} exceeds maximum allowed ({MAX_DAYS_TO_SCRAPE}). Setting to {MAX_DAYS_TO_SCRAPE} days.")
            self.days_to_scrape = MAX_DAYS_TO_SCRAPE

        self.img_size: str = img_size if img_size in ["150", "300", "600"] else DEFAULT_IMG_SIZE
        self.check_img: bool = bool(check_img) if check_img is not None else DEFAULT_IMG_CHECK
        if self.check_img:
            logger.debug("Image URL validity checks (--img-check) are ENABLED. This may increase scraping time.")
        else:
            logger.debug("Image URL validity checks (--img-check) are DISABLED.")

        self.cache_dir: str = cache_dir_path if cache_dir_path else os.path.join(tempfile.gettempdir(), DEFAULT_CACHE_SUBDIR)

        if os.path.exists(self.cache_dir):
            if not os.path.isdir(self.cache_dir):
                logger.error(f"Cache path '{self.cache_dir}' exists but is not a directory. Please resolve this conflict.")
                raise NotADirectoryError(f"Cache path '{self.cache_dir}' exists but is not a directory.")
        else:
            os.makedirs(self.cache_dir, exist_ok=True)

        self.user_agents: List[str] = USER_AGENTS

        self.max_workers: int = max_workers
        self.pool_size: int = max(self.max_workers, 1) * 2

        self.retry_strategy: Retry = Retry(
            total=max_retries,
            backoff_factor=RETRY_BACKOFF_FACTOR,
            status_forcelist=RETRY_STATUS_FORCELIST,
            allowed_methods=["GET", "HEAD"],
            raise_on_status=False,
            respect_retry_after_header=True
        )
        self.session = requests.Session()
        adapter = HTTPAdapter(
            max_retries=self.retry_strategy,
            pool_connections=self.pool_size,
            pool_maxsize=self.pool_size
        )
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        logger.debug("Created a single requests.Session for the scraper instance.")
        logger.debug(f"Main session retry strategy configured: Max retries={max_retries}, Backoff factor={RETRY_BACKOFF_FACTOR}, Retrying on status codes={RETRY_STATUS_FORCELIST}, respecting Retry-After header.")
        logger.debug(f"HTTPAdapter connection pool size set to: {self.pool_size}")

        self.min_request_delay: float = min_request_delay
        self.max_schedule_retries: int = max_schedule_retries

        self.enable_cache: bool = not disable_cache
        self.cache_ttl: int = cache_ttl
        self.processed_data_cache_base_dir: str = os.path.join(self.cache_dir, CACHE_PROCESSED_DATA_SUBDIR)
        self.keep_past_cache: bool = bool(keep_past_cache)
        self.cache_clear_requested: bool = cache_clear
        self.cache_validation_tolerance: int = cache_validation_tolerance
        self.img_crop_disable: bool = img_crop_disable
        self.http_timeout: int = http_timeout

        if self.enable_cache:
            os.makedirs(self.processed_data_cache_base_dir, exist_ok=True)
            logger.debug(f"Processed data cache enabled. Path: {self.processed_data_cache_base_dir}, TTL: {self.cache_ttl}s.")
            logger.debug("Conditional GET (ETag/Last-Modified/304) for cache consistency is ENABLED by default.")
            logger.debug(f"Content-Length tolerance for cache validation set to {self.cache_validation_tolerance} bytes.")
            if self.keep_past_cache:
                logger.debug("Keeping past days' cache files is ENABLED (--keep-past-cache).")
            else:
                logger.debug("Deleting past days' cache files is ENABLED by default (no --keep-past-cache).")
        else:
            logger.debug("Processed data cache disabled.")

        if self.cache_clear_requested:
            logger.info("Explicit --cache-clear requested. All caches will be invalidated/ignored.")

        self.stop_event = threading.Event()
        self.html_parser = TvsHtmlParser()
        self.xmltv_timezone: str = xmltv_timezone if xmltv_timezone else 'Europe/Berlin'

    def _build_schedule_url(self, channel_url: str, current_date: datetime.date) -> str:
        """
        Constructs the schedule URL for a given channel and date.

        :param channel_url: Base URL of the channel.
        :type channel_url: str
        :param current_date: The date for which to build the schedule URL.
        :type current_date: datetime.date
        :returns: The complete schedule URL.
        :rtype: str
        """
        date_str: str = current_date.strftime('%Y-%m-%d')
        return f"{channel_url}?date={date_str}"

    @functools.lru_cache(maxsize=256)
    def _make_request_cached_wrapper(self, method: str, url: str, headers_tuple: Tuple[Tuple[str, str], ...], allow_redirects: bool, timeout: int) -> Optional[requests.Response]:
        """
        A cached wrapper for _make_request. This method is decorated with lru_cache.
        Headers are passed as a hashable tuple of tuples.

        :param method: HTTP method ('get' or 'head').
        :type method: str
        :param url: The URL to fetch.
        :type url: str
        :param headers_tuple: Tuple of (header_name, header_value) tuples for headers.
        :type headers_tuple: Tuple[Tuple[str, str], ...]
        :param allow_redirects: Whether to follow HTTP redirects.
        :type allow_redirects: bool
        :param timeout: Request timeout in seconds.
        :type timeout: int
        :returns: The requests.Response object on success, None on failure.
        :rtype: Optional[requests.Response]
        """
        headers = dict(headers_tuple) 
        
        if self.stop_event.is_set():
            logger.debug(f"[Cached Wrapper] Stop event set. Aborting {method.upper()} for {url}.")
            return None

        parsed_url = urlparse(url)
        if not all([parsed_url.scheme, parsed_url.netloc]) or parsed_url.scheme not in ("http", "https"):
            logger.warning(f"[Cached Wrapper] Invalid or unsupported URL scheme: {url}. Skipping request.")
            return None

        try:
            if method == 'get':
                response: requests.Response = self.session.get(url, headers=headers, allow_redirects=allow_redirects, timeout=timeout)
            elif method == 'head':
                response: requests.Response = self.session.head(url, headers=headers, allow_redirects=allow_redirects, timeout=timeout)
            else:
                logger.error(f"[Cached Wrapper] Unsupported HTTP method: {method}")
                return None

            if self.stop_event.is_set():
                logger.debug(f"[Cached Wrapper] Stop event set during {method.upper()} for {url}.")
                return None

            time.sleep(random.uniform(self.min_request_delay, self.min_request_delay + 0.2))

            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            logger.error(f"[Cached Wrapper] Request failed for {url} ({method.upper()}): {e}")
            return None

    def fetch_url(self, url: str, etag: Optional[str] = None, last_modified: Optional[str] = None, cached_content_length: Optional[int] = None, allow_redirects: bool = True) -> Tuple[Optional[str], Optional[str], Optional[str], Optional[int]]:
        """
        Sends an HTTP GET request to the given URL using the configured session.

        Handles HTTP status codes, timeouts, and retries. Applies a delay only for
        live (non-cached) requests to be polite. Implements Conditional GET using ETag and Last-Modified headers.
        If a 304 Not Modified is not received, it checks content-length with a tolerance.

        :param url: The URL to fetch.
        :type url: str
        :param etag: The ETag from a previous response, for conditional GET.
        :type etag: Optional[str]
        :param last_modified: The Last-Modified header from a previous response, for conditional GET.
        :type last_modified: Optional[str]
        :param cached_content_length: The Content-Length from the last cached response.
        :type cached_content_length: Optional[int]
        :param allow_redirects: Whether to follow HTTP redirects. Defaults to True.
        :type allow_redirects: bool
        :returns: A tuple (response_content, new_etag, new_last_modified, fetched_content_length).
                  response_content is None if 304 Not Modified or if content is semantically identical within tolerance.
                  fetched_content_length is for the *newly fetched* content.
                  Returns (None, None, None, None) if request fails.
        :rtype: Tuple[Optional[str], Optional[str], Optional[str], Optional[int]]
        """
        if self.stop_event.is_set():
            logger.debug(f"Stop event set. Aborting fetch for {url}.")
            return None, None, None, None

        headers: Dict[str, str] = {'User-Agent': random.choice(self.user_agents)}

        if self.cache_clear_requested:
            logger.debug(f"Performing full GET for {url} (explicit --cache-clear requested).")
            response = self._make_request_uncached_internal('get', url, headers, allow_redirects, self.http_timeout)
            if response is None:
                return None, None, None, None

            new_etag: Optional[str] = response.headers.get('ETag')
            new_last_modified: Optional[str] = response.headers.get('Last-Modified')
            fetched_content_length: int = len(response.content)

            logger.debug(f"Fetched live: {url} (Status: {response.status_code}). ETag: {new_etag}, Last-Modified: {new_last_modified}, Content-Length: {fetched_content_length}")
            return response.text, new_etag, new_last_modified, fetched_content_length

        if etag:
            headers['If-None-Match'] = etag
        if last_modified:
            headers['If-Modified-Since'] = last_modified

        headers_tuple = tuple(sorted(headers.items()))
        response = self._make_request_cached_wrapper('get', url, headers_tuple, allow_redirects, self.http_timeout)

        if response is None:
            return None, None, None, None

        new_etag: Optional[str] = response.headers.get('ETag')
        new_last_modified: Optional[str] = response.headers.get('Last-Modified')
        fetched_content_length: int = len(response.content)

        if response.status_code == 304:
            logger.debug(f"Resource {url} not modified (304). Using cached data.")
            return None, etag, last_modified, cached_content_length

        if cached_content_length is not None and \
           abs(fetched_content_length - cached_content_length) <= self.cache_validation_tolerance:
            logger.debug(f"Resource {url} content length is within tolerance ({self.cache_validation_tolerance} bytes). "
                         f"Fetched: {fetched_content_length}, Cached: {cached_content_length}. Using cached data.")
            return None, new_etag, new_last_modified, fetched_content_length
        else:
            if cached_content_length is not None:
                logger.debug(f"Resource {url} content length differs beyond tolerance. "
                             f"Fetched: {fetched_content_length}, Cached: {cached_content_length}, Tolerance: {self.cache_validation_tolerance}. Fetching new content.")
            else:
                logger.debug(f"No cached content length for {url} or conditional GET failed. Fetching new content.")
            return response.text, new_etag, new_last_modified, fetched_content_length

    def _make_request_uncached_internal(self, method: str, url: str, headers: Dict[str, str], allow_redirects: bool, timeout: int) -> Optional[requests.Response]:
        """
        Internal function to make HTTP requests without caching and handle RequestExceptions.
        Used when lru_cache needs to be explicitly bypassed.

        :param method: HTTP method ('get' or 'head').
        :type method: str
        :param url: The URL to fetch.
        :type url: str
        :param headers: Dictionary of HTTP headers.
        :type headers: Dict[str, str]
        :param allow_redirects: Whether to follow HTTP redirects.
        :type allow_redirects: bool
        :param timeout: Request timeout in seconds.
        :type timeout: int
        :returns: The requests.Response object on success, None on failure.
        :rtype: Optional[requests.Response]
        """
        if self.stop_event.is_set():
            logger.debug(f"[Uncached Internal] Stop event set. Aborting {method.upper()} for {url}.")
            return None

        parsed_url = urlparse(url)
        if not all([parsed_url.scheme, parsed_url.netloc]) or parsed_url.scheme not in ("http", "https"):
            logger.warning(f"[Uncached Internal] Invalid or unsupported URL scheme: {url}. Skipping request.")
            return None

        try:
            if method == 'get':
                response: requests.Response = self.session.get(url, headers=headers, allow_redirects=allow_redirects, timeout=timeout)
            elif method == 'head':
                response: requests.Response = self.session.head(url, headers=headers, allow_redirects=allow_redirects, timeout=timeout)
            else:
                logger.error(f"[Uncached Internal] Unsupported HTTP method: {method}")
                return None

            if self.stop_event.is_set():
                logger.debug(f"[Uncached Internal] Stop event set during {method.upper()} for {url}.")
                return None

            time.sleep(random.uniform(self.min_request_delay, self.min_request_delay + 0.2))

            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            logger.error(f"[Uncached Internal] Request failed for {url} ({method.upper()}): {e}")
            return None


    def _get_channel_list(self) -> List[Dict[str, str]]:
        """
        Fetches and parses the list of channels from TVSpielfilm.de.
        Utilizes caching with ETag/Last-Modified for efficient fetching.

        :returns: A list of dictionaries, each containing 'name', 'url', and 'source_id' for a channel.
        :rtype: List[Dict[str, str]]
        """
        if self.stop_event.is_set():
            logger.debug("Stop event set. Aborting _get_channel_list.")
            return []

        channels: List[Dict[str, str]] = []
        new_etag: Optional[str] = None
        new_last_modified: Optional[str] = None
        fetched_content_length: Optional[int] = None

        cached_data, cached_etag, cached_last_modified, cached_content_length, is_cache_fresh = (
            None, None, None, None, False
        )

        if self.enable_cache and not self.cache_clear_requested:
            cached_data, cached_etag, cached_last_modified, cached_content_length, is_cache_fresh = self._load_channel_list_cache()
            
            if is_cache_fresh and cached_data is not None:
                channels = cached_data
                logger.debug("Using fresh channel list from JSON cache.")
                new_etag = cached_etag
                new_last_modified = cached_last_modified
                fetched_content_length = cached_content_length
                return channels
            elif cached_data is not None:
                logger.debug("Channel list JSON cache is stale. Attempting conditional fetch.")
            else:
                logger.debug("Channel list JSON cache not found. Performing full fetch.")
        else:
            if self.cache_clear_requested:
                logger.debug("Cache clear requested. Bypassing channel list cache.")
            elif not self.enable_cache:
                logger.debug("Cache disabled. Bypassing channel list cache.")

        logger.info(f"Fetching channel list from {CHANNEL_LIST_URL}")
        response_content, new_etag, new_last_modified, fetched_content_length = self.fetch_url(
            CHANNEL_LIST_URL,
            etag=cached_etag,
            last_modified=cached_last_modified,
            cached_content_length=cached_content_length
        )

        if response_content is not None:
            logger.debug("Channel list HTML content was modified or fetched for the first time. Parsing new data.")
            channels = self.html_parser.parse_channel_list(response_content)
            if self.enable_cache:
                self._save_channel_list_cache(channels, new_etag, new_last_modified, fetched_content_length)
        elif cached_data is not None:
            logger.debug("Channel list HTML content not modified (304) or semantically identical. Using stale JSON cache.")
            channels = cached_data
            new_etag = cached_etag
            new_last_modified = cached_last_modified
            fetched_content_length = fetched_content_length if fetched_content_length is not None else cached_content_length
        else:
            logger.error(f"Failed to fetch channel list from {CHANNEL_LIST_URL} and no cache available.")

        return channels

    def _get_cache_channel_list_filepath(self) -> str:
        """
        Constructs the file path for the channel list JSON cache.

        :returns: The full path to the JSON file.
        :rtype: str
        """
        return os.path.join(self.processed_data_cache_base_dir, CACHE_CHANNEL_LIST_FILE)

    def _load_channel_list_cache(self) -> Tuple[Optional[List[Dict[str, Any]]], Optional[str], Optional[str], Optional[int], bool]:
        """
        Loads cached channel list data along with ETag, Last-Modified, and Content-Length.
        Checks if the data is fresh based on its internal timestamp and TTL.
        This function now always attempts to load the full JSON content if the file exists,
        to retrieve ETag and Last-Modified headers for a potential conditional GET.

        :returns: A tuple containing cached data, ETag, Last-Modified header, content length, and freshness status.
        :rtype: Tuple[Optional[List[Dict[str, Any]]], Optional[str], Optional[str], Optional[int], bool]
        """
        if self.stop_event.is_set():
            logger.debug("Stop event set. Aborting cache load for channel list.")
            return None, None, None, None, False

        filepath: str = self._get_cache_channel_list_filepath()
        if not os.path.exists(filepath):
            logger.debug(f"Channel list cache file not found at {filepath}.")
            return None, None, None, None, False

        try:
            with open(filepath, 'r+', encoding='utf-8') as f:
                portalocker.lock(f, portalocker.LOCK_SH)
                try:
                    cache_content: Dict[str, Any] = json.load(f)
                except json.JSONDecodeError as e:
                    logger.warning(f"Error decoding channel list JSON cache: {e}. Deleting corrupted file and re-scraping: {filepath}")
                    f.seek(0)
                    f.truncate()
                    if os.path.exists(filepath):
                        os.remove(filepath)
                    return None, None, None, None, False
                finally:
                    portalocker.unlock(f)

            data: Optional[List[Dict[str, Any]]] = cache_content.get('data')
            etag: Optional[str] = cache_content.get('etag')
            last_modified: Optional[str] = cache_content.get('last_modified')
            cached_at_str: Optional[str] = cache_content.get('cached_at')
            content_length: Optional[int] = cache_content.get('content_length')
            if content_length is None:
                content_length = 0

            is_fresh: bool = False
            if cached_at_str:
                cached_at: datetime = datetime.fromisoformat(cached_at_str)
                current_time: datetime = datetime.now(timezone.utc) if cached_at.tzinfo else datetime.now()
                if (current_time - cached_at.replace(tzinfo=timezone.utc)).total_seconds() < self.cache_ttl:
                    is_fresh = True

            if is_fresh:
                logger.debug("Loaded channel list from JSON cache (fresh).")
            else:
                logger.debug("Channel list JSON cache is stale. Will attempt conditional fetch.")

            return data, etag, last_modified, content_length, is_fresh

        except portalocker.LockException as e:
            logger.warning(f"Could not acquire lock for channel list cache: {e}. Skipping cache load.")
            return None, None, None, None, False
        except FileNotFoundError:
            logger.debug(f"Channel list cache file not found at {filepath}.")
            return None, None, None, None, False
        except IOError as e:
            logger.warning(f"IOError loading channel list JSON cache: {e}. Will attempt re-scraping.")
            return None, None, None, None, False
        except Exception as e:
            logger.error(f"Unexpected error in _load_channel_list_cache: {e}")
            return None, None, None, None, False

    def _save_channel_list_cache(self, data: List[Dict[str, Any]], etag: Optional[str] = None, last_modified: Optional[str] = None, content_length: Optional[int] = None) -> None:
        """
        Saves channel list data to the JSON cache, including HTTP ETag, Last-Modified, and Content-Length headers,
        using an atomic write operation with a temporary file.

        :param data: The channel list data to save.
        :type data: List[Dict[str, Any]]
        :param etag: The ETag header value.
        :type etag: Optional[str]
        :param last_modified: The Last-Modified header value.
        :type last_modified: Optional[str]
        :param content_length: The Content-Length of the fetched content.
        :type content_length: Optional[int]
        """
        if self.stop_event.is_set():
            logger.debug("Stop event set. Aborting cache save for channel list.")
            return

        filepath: str = self._get_cache_channel_list_filepath()
        temp_filepath: str = f"{filepath}.tmp.{os.getpid()}"

        try:
            with open(temp_filepath, 'w+', encoding='utf-8') as f:
                portalocker.lock(f, portalocker.LOCK_EX)
                try:
                    cache_content: Dict[str, Any] = {
                        'data': data,
                        'etag': etag,
                        'last_modified': last_modified,
                        'cached_at': datetime.now(timezone.utc).isoformat(),
                        'content_length': content_length,
                    }
                    json.dump(cache_content, f, ensure_ascii=False, indent=4)
                    f.flush()
                    os.fsync(f.fileno())
                finally:
                    portalocker.unlock(f)

            os.replace(temp_filepath, filepath)
            logger.debug(f"Saved channel list to JSON cache: {filepath}")

        except portalocker.LockException as e:
            logger.warning(f"Could not acquire lock for temporary channel list cache during save: {e}. Skipping cache save.")
            if os.path.exists(temp_filepath):
                os.remove(temp_filepath)
        except IOError as e:
            logger.error(f"Error saving channel list JSON cache (IOError): {e}.")
            if os.path.exists(temp_filepath):
                os.remove(temp_filepath)
        except Exception as e:
            logger.error(f"Unexpected error saving channel list JSON cache: {e}")
            if os.path.exists(temp_filepath):
                os.remove(temp_filepath)


    def _get_daily_cache_filepath(self, channel_id: str, date: datetime.date) -> str:
        """
        Constructs the file path for the daily JSON data cache.

        :param channel_id: The ID of the channel.
        :type channel_id: str
        :param date: The date of the schedule.
        :type date: datetime.date
        :returns: The full path to the JSON file.
        :rtype: str
        """
        channel_dir: str = os.path.join(self.processed_data_cache_base_dir, channel_id.lower())
        os.makedirs(channel_dir, exist_ok=True)
        return os.path.join(channel_dir, f"{date.strftime('%Y%m%d')}.json")

    def _load_daily_json_cache(self, channel_id: str, date: datetime.date) -> Tuple[Optional[List[Dict[str, Any]]], Optional[str], Optional[str], Optional[int], Optional[str], bool]:
        """
        Loads processed JSON data from the daily data cache, along with ETag, Last-Modified, Content-Length,
        and the timezone it was cached with.
        Checks if the data is fresh based on its internal timestamp and TTL.
        Also deletes cache files for past days (if --keep-past-cache is not set) and for dates
        beyond MAX_DAYS_TO_SCRAPE from the current date.
        This function now always attempts to load the full JSON content if the file exists,
        to retrieve ETag and Last-Modified headers for a potential conditional GET.

        :param channel_id: The ID of the channel.
        :type channel_id: str
        :param date: The date of the schedule.
        :type date: datetime.date
        :returns: A tuple (data, etag, last_modified, content_length, cached_xmltv_timezone, is_fresh).
                  data is None if file not found or corrupted.
                  is_fresh is True if the data is within TTL.
        :rtype: Tuple[Optional[List[Dict[str, Any]]], Optional[str], Optional[str], Optional[int], Optional[str], bool]
        """
        if self.stop_event.is_set():
            logger.debug(f"Stop event set. Aborting cache load for {channel_id} on {date.strftime('%Y-%m-%d')}.")
            return None, None, None, None, None, False

        filepath: str = self._get_daily_cache_filepath(channel_id, date)
        if not os.path.exists(filepath):
            logger.debug(f"Daily JSON cache file not found for {channel_id} on {date.strftime('%Y-%m-%d')} at {filepath}.")
            return None, None, None, None, None, False

        if not self.keep_past_cache and date < (datetime.now().date() - timedelta(days=1)):
            logger.debug(f"Deleting stale cache file for past day {date.strftime('%Y-%m-%d')} for channel {channel_id} as --keep-past-cache is not set.")
            try:
                os.remove(filepath)
            except OSError as e:
                logger.error(f"Error deleting cache file {filepath}: {e}")
            return None, None, None, None, None, False

        max_relevant_date = datetime.now().date() + timedelta(days=MAX_DAYS_TO_SCRAPE - 1)
        if date > max_relevant_date:
            logger.debug(f"Deleting outdated cache file for {date.strftime('%Y-%m-%d')} for channel {channel_id} (beyond max relevant date).")
            try:
                os.remove(filepath)
            except OSError as e:
                logger.error(f"Error deleting cache file {filepath}: {e}")
            return None, None, None, None, None, False

        try:
            with open(filepath, 'r+', encoding='utf-8') as f:
                portalocker.lock(f, portalocker.LOCK_SH)
                try:
                    cache_content: Dict[str, Any] = json.load(f)
                except json.JSONDecodeError as e:
                    logger.warning(f"Error decoding daily JSON cache for {channel_id} on {date.strftime('%Y-%m-%d')}: {e}. Deleting corrupted file and re-scraping: {filepath}")
                    f.seek(0)
                    f.truncate()
                    if os.path.exists(filepath):
                        os.remove(filepath)
                    return None, None, None, None, None, False
                finally:
                    portalocker.unlock(f)

            data: Optional[List[Dict[str, Any]]] = cache_content.get('data')
            etag: Optional[str] = cache_content.get('etag')
            last_modified: Optional[str] = cache_content.get('last_modified')
            cached_at_str: Optional[str] = cache_content.get('cached_at')
            content_length: Optional[int] = cache_content.get('content_length')
            if content_length is None:
                content_length = 0
            cached_xmltv_timezone: Optional[str] = cache_content.get('xmltv_timezone')

            is_fresh: bool = False
            if cached_at_str:
                cached_at: datetime = datetime.fromisoformat(cached_at_str)
                current_time: datetime = datetime.now(timezone.utc)
                if (current_time - cached_at.replace(tzinfo=timezone.utc)).total_seconds() < self.cache_ttl:
                    is_fresh = True

            if is_fresh:
                logger.debug(f"Loaded schedule for {channel_id} on {date.strftime('%Y-%m-%d')} from daily JSON cache (fresh).")
            else:
                logger.debug(f"Daily JSON cache for {channel_id} on {date.strftime('%Y-%m-%d')} is stale. Will attempt conditional fetch.")

            return data, etag, last_modified, content_length, cached_xmltv_timezone, is_fresh

        except portalocker.LockException as e:
            logger.warning(f"Could not acquire lock for daily cache file {filepath}: {e}. Skipping cache load.")
            return None, None, None, None, None, False
        except FileNotFoundError:
            logger.debug(f"Daily JSON cache file not found for {channel_id} on {date.strftime('%Y-%m-%d')} at {filepath}.")
            return None, None, None, None, None, False
        except IOError as e:
            logger.warning(f"IOError loading daily JSON cache for {channel_id} on {date.strftime('%Y-%m-%d')}: {e}. Will attempt re-scraping.")
            return None, None, None, None, False
        except Exception as e:
            logger.error(f"Unexpected error in _load_daily_json_cache for {channel_id} on {date.strftime('%Y-%m-%d')}: {e}")
            return None, None, None, None, None, False

    def _save_daily_json_cache(self, channel_id: str, date: datetime.date, data: List[Dict[str, Any]], etag: Optional[str] = None, last_modified: Optional[str] = None, content_length: Optional[int] = None, xmltv_timezone_used: Optional[str] = None) -> None:
        """
        Saves processed JSON data to the daily data cache, including HTTP ETag, Last-Modified,
        Content-Length headers, and the timezone used for XMLTV time formatting,
        using an atomic write operation with a temporary file.

        :param channel_id: The ID of the channel.
        :type channel_id: str
        :param date: The date of the schedule.
        :type date: datetime.date
        :param data: The program data for the day to save.
        :type data: List[Dict[str, Any]]
        :param etag: The ETag header value.
        :type etag: Optional[str]
        :param last_modified: The Last-Modified header value.
        :type last_modified: Optional[str]
        :param content_length: The Content-Length of the fetched content.
        :type content_length: Optional[int]
        :param xmltv_timezone_used: The timezone string used when generating 'starttime_xmltv' and 'endtime_xmltv'.
        :type xmltv_timezone_used: Optional[str]
        """
        if self.stop_event.is_set():
            logger.debug(f"Stop event set. Aborting cache save for {channel_id} on {date.strftime('%Y-%m-%d')}.")
            return

        filepath: str = self._get_daily_cache_filepath(channel_id, date)
        temp_filepath: str = f"{filepath}.tmp.{os.getpid()}"

        try:
            with open(temp_filepath, 'w+', encoding='utf-8') as f:
                portalocker.lock(f, portalocker.LOCK_EX)
                try:
                    cache_content: Dict[str, Any] = {
                        'data': data,
                        'etag': etag,
                        'last_modified': last_modified,
                        'cached_at': datetime.now(timezone.utc).isoformat(),
                        'content_length': content_length,
                        'xmltv_timezone': xmltv_timezone_used
                    }
                    json.dump(cache_content, f, ensure_ascii=False, indent=4)
                    f.flush()
                    os.fsync(f.fileno())
                finally:
                    portalocker.unlock(f)

            os.replace(temp_filepath, filepath)
            logger.debug(f"Saved schedule for {channel_id} on {date.strftime('%Y-%m-%d')} to daily JSON cache.")

        except portalocker.LockException as e:
            logger.warning(f"Could not acquire lock for temporary daily cache file during save: {e}. Skipping cache save.")
            if os.path.exists(temp_filepath):
                os.remove(temp_filepath)
        except IOError as e:
            logger.error(f"Error saving daily JSON cache for {channel_id} on {date.strftime('%Y-%m-%d')} (IOError): {e}.")
            if os.path.exists(temp_filepath):
                os.remove(temp_filepath)
        except Exception as e:
            logger.error(f"Unexpected error saving daily JSON cache for {channel_id} on {date.strftime('%Y-%m-%d')}: {e}")
            if os.path.exists(temp_filepath):
                os.remove(temp_filepath)

    def _strip_im_parameter_for_cache(self, url: str) -> str:
        """
        Strips the 'im' parameter from a URL for saving to JSON cache.
        This function always removes the 'im' parameter.

        :param url: The original URL.
        :type url: str
        :returns: The URL with 'im' parameter removed.
        :rtype: str
        """
        parsed = urlparse(url)
        query_params = parse_qs(parsed.query)

        if 'im' in query_params:
            new_query_parts = []
            for key, values in query_params.items():
                if key != 'im':
                    for value in values:
                        new_query_parts.append(f"{key}={value}")
            new_query_string = '&'.join(new_query_parts)
            return parsed._replace(query=new_query_string).geturl()
        return url

    def _check_image_url_head_threaded(self, original_image_url: str, item_id_for_log: str) -> Tuple[str, str]:
        """
        Performs a HEAD request to check if an image URL is valid and returns its final URL after redirects.
        Designed to be called in a threaded context.

        :param original_image_url: The original image URL to check.
        :type original_image_url: str
        :param item_id_for_log: A unique identifier for logging purposes for the current item.
        :type item_id_for_log: str
        :returns: A tuple (final_url, status). 'final_url' is the URL after redirects or original if no redirect/error.
                  'status' is 'avail' if OK (200), 'not_avail' if 4xx/5xx, 'redirect' if redirect to placeholder, 'error' for others.
        :rtype: Tuple[str, str]
        """
        if self.stop_event.is_set():
            logger.debug(f"Stop event set. Aborting image check for {item_id_for_log}.")
            return original_image_url, 'error'

        final_url: str = original_image_url
        status: str = 'error'

        try:
            headers = {'User-Agent': random.choice(self.user_agents)}
            headers_tuple = tuple(sorted(headers.items()))
            response = self._make_request_cached_wrapper('head', original_image_url, headers_tuple, allow_redirects=True, timeout=self.http_timeout)

            if response:
                final_url = response.url
                if response.status_code == 200:
                    status = 'avail'
                    logger.debug(f"Image {item_id_for_log} is available: {final_url}")
                elif 400 <= response.status_code < 500:
                    status = 'not_avail'
                    logger.warning(f"Image {item_id_for_log} not available (HTTP {response.status_code}): {final_url}")
                elif 500 <= response.status_code < 600:
                    status = 'not_avail'
                    logger.warning(f"Image {item_id_for_log} not available (HTTP {response.status_code}): {final_url}")
                else:
                    status = 'error'
                    logger.warning(f"Image {item_id_for_log} check returned unexpected status {response.status_code}: {final_url}")
            else:
                status = 'error'
                logger.warning(f"Failed HEAD request for image {item_id_for_log}: {original_image_url}")

        except Exception as e:
            status = 'error'
            logger.warning(f"Exception during HEAD request for image {item_id_for_log}: {e}. URL: {original_image_url}")

        if status == 'avail' and (
            'placehold.it' in final_url or
            'placeholder.com' in final_url or
            'dummyimage.com' in final_url or
            'empty.jpg' in final_url.lower() or
            'noimage' in final_url.lower()
        ):
            status = 'redirect'
            logger.info(f"Image {item_id_for_log} redirected to a generic placeholder: {final_url}")

        return final_url, status

    def parse_detail_page_threaded(self, detail_url: str, detail_data_template: Dict[str, Any]) -> Dict[str, Any]:
        """
        Parses the detail page of a program to extract additional information.
        Designed to be called in a threaded context.

        :param detail_url: The URL of the program's detail page.
        :type detail_url: str
        :param detail_data_template: A template dictionary for initializing detail data.
        :type detail_data_template: Dict[str, Any]
        :returns: A dictionary containing detailed program information.
        :rtype: Dict[str, Any]
        """
        if self.stop_event.is_set():
            logger.debug(f"Stop event set. Aborting detail page parsing for {detail_url}.")
            return self._DETAIL_DATA_TEMPLATE.copy()

        response_content, _, _, _ = self.fetch_url(detail_url)

        if not response_content:
            logger.warning(f"Failed to fetch detail page content: {detail_url}. Skipping detail data extraction.")
            return self._DETAIL_DATA_TEMPLATE.copy()

        return self.html_parser.parse_program_details(response_content, detail_data_template)

    def _fetch_detail_and_image_threaded(self, detail_url: str, detail_data_template: Dict[str, Any], original_image_url: str, item_id_for_log: str) -> Tuple[Dict[str, Any], str, str]:
        """
        Combines detail page parsing and image head check into a single threaded task.

        :param detail_url: The URL of the program's detail page.
        :type detail_url: str
        :param detail_data_template: A template dictionary for initializing detail data.
        :type detail_data_template: Dict[str, Any]
        :param original_image_url: The original image URL to check.
        :type original_image_url: str
        :param item_id_for_log: A unique identifier for logging purposes for the current item.
        :type item_id_for_log: str
        :returns: A tuple (detail_data, final_image_url, image_status).
        :rtype: Tuple[Dict[str, Any], str, str]
        """
        detail_data = self.parse_detail_page_threaded(detail_url, detail_data_template)
        checked_url, status = self._check_image_url_head_threaded(original_image_url, item_id_for_log)
        return detail_data, checked_url, status


    def _process_channel_day_schedule(self, channel_info: Dict[str, str], current_date: datetime.date) -> List[Dict[str, Any]]:
        """
        Fetches and parses a day's schedule for a channel, with retry logic for
        application-level exceptions during schedule parsing.

        It prioritizes fresh data from the local JSON cache. If stale or missing,
        it performs a conditional HTTP GET to check for updates. If cached data's
        timezone differs from requested, it reprocesses timestamps.

        :param channel_info: Dictionary containing channel information ('name', 'url', 'source_id').
        :type channel_info: Dict[str, str]
        :param current_date: The date for which the schedule is being parsed.
        :type current_date: datetime.date
        :returns: A list of dictionaries, each representing a program item.
        :rtype: List[Dict[str, Any]]
        """
        if self.stop_event.is_set():
            logger.debug(f"Stop event set. Aborting _process_channel_day_schedule for {channel_info['name']} on {current_date.strftime('%Y-%m-%d')}.")
            return []

        channel_name: str = channel_info['name']
        channel_url: str = channel_info['url']
        channel_source_id: str = channel_info['source_id']

        items_for_day: List[Dict[str, Any]] = []
        processed_items: List[Dict[str, Any]] = []
        new_etag: Optional[str] = None
        new_last_modified: Optional[str] = None
        fetched_content_length: Optional[int] = None

        cached_data, cached_etag, cached_last_modified, cached_content_length, cached_xmltv_timezone, is_cache_fresh = (
            None, None, None, None, None, False
        )

        if self.enable_cache and not self.cache_clear_requested:
            cached_data, cached_etag, cached_last_modified, cached_content_length, cached_xmltv_timezone, is_cache_fresh = (
                self._load_daily_json_cache(channel_source_id, current_date)
            )

            if is_cache_fresh and cached_data is not None:
                if cached_xmltv_timezone == self.xmltv_timezone:
                    items_for_day = cached_data
                    logger.debug(f"Using fresh data from JSON cache for {channel_name} on {current_date.strftime('%Y-%m-%d')} (timezone matches).")
                else:
                    logger.info(f"Cached data for {channel_name} on {current_date.strftime('%Y-%m-%d')} is fresh but timezone differs (cached: {cached_xmltv_timezone}, requested: {self.xmltv_timezone}). Reprocessing timestamps.")
                    items_for_day = cached_data
                new_etag = cached_etag
                new_last_modified = cached_last_modified
                fetched_content_length = cached_content_length
        else:
            if self.cache_clear_requested:
                logger.debug(f"Cache clear requested. Bypassing cache load for {channel_name} on {current_date.strftime('%Y-%m-%d')}.")
            elif not self.enable_cache:
                logger.debug(f"Cache disabled. Bypassing cache load for {channel_name} on {current_date.strftime('%Y-%m-%d')}.")

        if not items_for_day or not is_cache_fresh:
            schedule_url: str = self._build_schedule_url(channel_url, current_date)
            for attempt in range(self.max_schedule_retries + 1):
                if self.stop_event.is_set():
                    logger.debug(f"Stop event set. Aborting schedule fetch attempts for {channel_name} on {current_date.strftime('%Y-%m-%d')}.")
                    return []
                try:
                    attempt_str: str = f" (Attempt {attempt + 1}/{self.max_schedule_retries + 1})" if attempt > 0 else ""
                    logger.debug(f"Attempting to fetch schedule for {channel_name} on {current_date.strftime('%Y-%m-%d')} from {schedule_url}{attempt_str}")

                    response_content, new_etag, new_last_modified, fetched_content_length = self.fetch_url(
                        schedule_url,
                        etag=cached_etag,
                        last_modified=cached_last_modified,
                        cached_content_length=cached_content_length
                    )

                    if response_content is not None:
                        logger.debug(f"HTML content for {channel_name} on {current_date.strftime('%Y-%m-%d')} was modified or fetched for the first time. Parsing new data.")
                        items_for_day = self.html_parser.parse_daily_schedule_items(
                            response_content,
                            channel_info,
                            current_date
                        )
                        break
                    elif cached_data is not None:
                        logger.debug(f"HTML content for {channel_name} on {current_date.strftime('%Y-%m-%d')} not modified (304) or semantically identical. Using stale JSON cache.")
                        items_for_day = cached_data
                        new_etag = cached_etag
                        new_last_modified = cached_last_modified
                        fetched_content_length = fetched_content_length if fetched_content_length is not None else cached_content_length
                        break
                    else:
                        logger.warning(f"Could not retrieve or parse program for {channel_name} on {current_date.strftime('%Y-%m-%d')}. No data available. Retrying...")
                        if attempt < self.max_schedule_retries:
                            time.sleep(1 * (attempt + 1))
                        else:
                            logger.error(f"Max retries ({self.max_schedule_retries}) exhausted for schedule generation for {channel_name} on {current_date.strftime('%Y-%m-%d')}. Skipping this day.")
                            return []
                except KeyboardInterrupt:
                    self.stop_event.set()
                    logger.info(f"KeyboardInterrupt caught in worker for {channel_name} on {current_date.strftime('%Y-%m-%d')}. Signaling main thread to stop.")
                    raise
                except Exception as exc:
                    logger.exception(f"Schedule generation for {channel_name} on {current_date.strftime('%Y-%m-%d')} generated an exception on attempt {attempt + 1}: {exc}")
                    if attempt < self.max_schedule_retries:
                        time.sleep(1 * (attempt + 1))
                    else:
                        logger.error(f"Max retries ({self.max_schedule_retries}) exhausted for schedule generation for {channel_name} on {current_date.strftime('%Y-%m-%d')}. Skipping this day.")
                        return []

        if self.stop_event.is_set():
            return []

        detail_tasks = []
        image_check_tasks = []
        combined_detail_image_tasks = [] 

        for index, item_data in enumerate(items_for_day):
            needs_detail = item_data.get('link') and not item_data.get('longdescription')
            needs_image_check = self.check_img and item_data.get('image_url') and item_data.get('image_check') != 'avail'

            if needs_detail and needs_image_check:
                combined_detail_image_tasks.append((index, item_data['link'], item_data['image_url'], item_data.get('eventid', 'N/A')))
            elif needs_detail:
                detail_tasks.append((index, item_data['link']))
            elif needs_image_check:
                item_id_for_log = f"{channel_source_id}-{current_date.strftime('%Y%m%d')}-{item_data.get('starttime', 'XX')}-{self.html_parser.re_log_item_id_cleanup.sub('', item_data.get('title', ''))[:10]}"
                image_check_tasks.append((index, item_data['image_url'], item_id_for_log))
            
            if not self.check_img and item_data.get('image_url'):
                 item_data['image_check'] = 'not_checked'

            if item_data.get('image_url'):
                item_data['image_url'] = self._strip_im_parameter_for_cache(item_data['image_url'])

        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as detail_executor, \
             concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers * 2) as image_executor:
            all_futures = []
            future_to_task_map = {}

            for index, detail_url in detail_tasks:
                future = detail_executor.submit(self.parse_detail_page_threaded, detail_url, self._DETAIL_DATA_TEMPLATE.copy())
                all_futures.append(future)
                future_to_task_map[future] = ('detail_only', index)

            for index, image_url, item_id_for_log in image_check_tasks:
                future = image_executor.submit(self._check_image_url_head_threaded, image_url, item_id_for_log)
                all_futures.append(future)
                future_to_task_map[future] = ('image_only', index)

            for index, detail_url, image_url, item_id_for_log in combined_detail_image_tasks:
                future = detail_executor.submit(self._fetch_detail_and_image_threaded, detail_url, self._DETAIL_DATA_TEMPLATE.copy(), image_url, item_id_for_log)
                all_futures.append(future)
                future_to_task_map[future] = ('detail_and_image', index)

            for future in concurrent.futures.as_completed(all_futures):
                if self.stop_event.is_set():
                    logger.info("Stop event detected. Shutting down executors and cancelling remaining tasks.")
                    detail_executor.shutdown(wait=False, cancel_futures=True)
                    image_executor.shutdown(wait=False, cancel_futures=True)
                    break

                task_type, index = future_to_task_map[future]
                try:
                    result = future.result()
                    if task_type == 'detail_only':
                        items_for_day[index].update(result)
                    elif task_type == 'image_only':
                        checked_url, status = result
                        items_for_day[index]['image_url'] = checked_url
                        items_for_day[index]['image_check'] = status
                    elif task_type == 'detail_and_image':
                        detail_page_data, checked_url, status = result
                        items_for_day[index].update(detail_page_data)
                        items_for_day[index]['image_url'] = checked_url
                        items_for_day[index]['image_check'] = status
                except Exception as exc:
                    logger.error(f"Error processing detail/image for item {items_for_day[index].get('title', 'N/A')} on {current_date.strftime('%Y-%m-%d')}: {exc}", exc_info=True)


        for item_data in items_for_day:
            if self.stop_event.is_set():
                logger.debug(f"Stop event set. Aborting final item processing for {channel_name}.")
                return []
            item_data['rating_cleaned'] = _process_rating_string(item_data.get('rating'))
            item_data['starttime_xmltv'] = _convert_unix_to_xmltv_time(item_data.get('starttime'), self.xmltv_timezone)
            item_data['endtime_xmltv'] = _convert_unix_to_xmltv_time(item_data.get('endtime'), self.xmltv_timezone)
            processed_items.append(item_data)

        if self.enable_cache:
            self._save_daily_json_cache(channel_source_id, current_date, processed_items, new_etag, new_last_modified, fetched_content_length, self.xmltv_timezone)
        return processed_items

    def _proactive_cache_cleanup(self) -> None:
        """
        Performs a proactive cleanup of old and irrelevant cache directories and files.
        This includes:
        - Deleting channel-specific subdirectories that are empty.
        - Deleting channel-specific subdirectories for channels that are no longer targeted.
        - Deleting daily JSON cache files for dates beyond MAX_DAYS_TO_SCRAPE from the current date.
        - Deleting daily JSON cache files for past days if --keep-past-cache is not set.
        - Deleting any orphaned temporary .tmp.* files within the cache directory structure.
        """
        logger.info("Starting proactive cache cleanup.")
        current_date = datetime.now().date()
        
        earliest_relevant_date = current_date - timedelta(days=1)
        latest_relevant_date = current_date + timedelta(days=MAX_DAYS_TO_SCRAPE - 1)

        if os.path.exists(self.processed_data_cache_base_dir):
            for channel_dir_name in os.listdir(self.processed_data_cache_base_dir):
                channel_dir_path = os.path.join(self.processed_data_cache_base_dir, channel_dir_name)
                if os.path.isdir(channel_dir_path):
                    if not os.listdir(channel_dir_path):
                        try:
                            os.rmdir(channel_dir_path)
                            logger.debug(f"Removed empty cache directory: {channel_dir_path}")
                        except OSError as e:
                            logger.warning(f"Failed to remove empty cache directory {channel_dir_path}: {e}")
                        continue

                    if self.target_sourceids and channel_dir_name.upper() not in self.target_sourceids:
                        logger.info(f"Removing cache directory for non-targeted channel: {channel_dir_name} at {channel_dir_path}")
                        try:
                            shutil.rmtree(channel_dir_path)
                        except OSError as e:
                            logger.error(f"Failed to remove cache directory for non-targeted channel {channel_dir_path}: {e}")
                        continue

                    for filename in os.listdir(channel_dir_path):
                        filepath = os.path.join(channel_dir_path, filename)
                        if filename.endswith('.json'):
                            file_date_str = filename.replace('.json', '')
                            try:
                                file_date = datetime.strptime(file_date_str, '%Y%m%d').date()
                                if (not self.keep_past_cache and file_date < earliest_relevant_date) or \
                                   file_date > latest_relevant_date:
                                    logger.debug(f"Deleting outdated cache file: {filepath}")
                                    try:
                                        os.remove(filepath)
                                    except OSError as e:
                                        logger.warning(f"Failed to delete outdated cache file {filepath}: {e}")
                            except ValueError:
                                logger.debug(f"Skipping non-date named file in cache: {filename}")
                        elif filename.startswith('.') and '.tmp.' in filename:
                            logger.info(f"Deleting orphaned temporary file: {filepath}")
                            try:
                                os.remove(filepath)
                            except OSError as e:
                                logger.warning(f"Failed to delete orphaned temporary file {filepath}: {e}")
                elif channel_dir_path.startswith('.') and '.tmp.' in channel_dir_path:
                    logger.info(f"Deleting orphaned temporary file (top-level): {channel_dir_path}")
                    try:
                        os.remove(channel_dir_path)
                    except OSError as e:
                        logger.warning(f"Failed to delete orphaned temporary file {channel_dir_path}: {e}")

        logger.info("Proactive cache cleanup completed.")


    def run_scraper(self) -> List[Dict[str, Any]]:
        """
        Orchestrates the main scraping process.

        Fetches the list of channels, then concurrently scrapes the schedule
        for each channel for the specified number of days.

        :returns: A list of all extracted program items.
        :rtype: List[Dict[str, Any]]
        """
        logger.info("Starting scrape process...")

        if not self.is_specific_date_set:
            logger.info(f"Scraping {self.days_to_scrape} day(s).")

        if self.enable_cache and not self.cache_clear_requested:
            self._proactive_cache_cleanup()

        available_channels_full_list: List[Dict[str, str]] = self._get_channel_list()
        if self.stop_event.is_set():
            return []
        available_channel_ids: set[str] = {channel['source_id'] for channel in available_channels_full_list}

        if self.target_sourceids:
            missing_ids: set[str] = self.target_sourceids - available_channel_ids
            for missing_id in missing_ids:
                logger.warning(f"Requested channel ID '{missing_id}' not found on TVSpielfilm.de. This channel will be skipped.")

            channel_list_to_scrape: List[Dict[str, str]] = [
                channel for channel in available_channels_full_list
                if channel['source_id'] in self.target_sourceids
            ]
            if not channel_list_to_scrape:
                logger.warning("No requested channels found on the website. Exiting.")
                return []
        else:
            channel_list_to_scrape = available_channels_full_list

        if not channel_list_to_scrape:
            logger.warning("No channels found or specified. Exiting.")
            return []

        total_channels: int = len(channel_list_to_scrape)

        all_extracted_items: List[Dict[str, Any]] = []

        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            for i, channel in enumerate(channel_list_to_scrape):
                if self.stop_event.is_set():
                    logger.info("Stop event detected. Aborting further channel processing.")
                    break

                channel_num: int = i + 1
                logger.info(f"Scraping channel: {channel['name']} (ID: {channel['source_id']}) [{channel_num}/{total_channels}]")

                channel_tasks: List[Tuple[Dict[str, str], datetime.date]] = []
                for j in range(self.days_to_scrape):
                    current_date: datetime.date = self.start_date + timedelta(days=j)
                    channel_tasks.append((channel, current_date))

                future_to_day_task: Dict[concurrent.futures.Future, Tuple[Dict[str, str], datetime.date]] = {
                    executor.submit(self._process_channel_day_schedule, ch, date): (ch, date)
                    for ch, date in channel_tasks
                }

                for future in concurrent.futures.as_completed(future_to_day_task):
                    if self.stop_event.is_set():
                        logger.info(f"Stop event detected for channel {channel['name']}. Cancelling remaining tasks for this channel and shutting down executor.")
                        executor.shutdown(wait=False, cancel_futures=True)
                        break

                    ch_info, current_date_for_task = future_to_day_task[future]
                    try:
                        items_for_day: List[Dict[str, Any]] = future.result()
                        all_extracted_items.extend(items_for_day)
                        logger.info(f"Finished processing items for {ch_info['name']} on {current_date_for_task.strftime('%Y-%m-%d')}. Extracted {len(items_for_day)} items.")
                    except concurrent.futures.CancelledError:
                        logger.info(f"Task for {ch_info['name']} on {current_date_for_task.strftime('%Y-%m-%d')} was cancelled.")
                    except Exception as exc:
                        logger.error(f"Error processing schedule for channel {ch_info['name']} on {current_date.strftime('%Y-%m-%d')}: {exc}", exc_info=True)

        logger.info(f"Scraping completed. Total items extracted: {len(all_extracted_items)}")

        return all_extracted_items

def _build_image_url_with_params(original_url: str, target_width: int, img_crop_disable: bool) -> str:
    """
    Constructs an image URL with specified resize and optional crop parameters for XMLTV output.

    If img_crop_disable is False, a default 16:9 crop (800x450) is added, replacing any existing crop.
    If img_crop_disable is True, any existing crop instructions are removed.
    The 'Resize,width={target_width}' parameter is always added or updated.

    :param original_url: The original image URL.
    :type original_url: str
    :param target_width: The desired width for resizing.
    :type target_width: int
    :param img_crop_disable: If True, disables default image cropping and removes any existing crop.
    :type img_crop_disable: bool
    :returns: The modified image URL.
    :rtype: str
    """
    parsed_url = urlparse(original_url)
    raw_query_string = parsed_url.query

    im_instructions_unencoded = []
    other_query_parts = []

    if raw_query_string:
        for part in raw_query_string.split('&'):
            if part.startswith('im='):
                im_value = parse_qs(part, keep_blank_values=True).get('im', [''])[0]
                for instr in im_value.split(';'):
                    if instr.strip() and not instr.strip().startswith('Crop,') and not instr.strip().startswith('Resize,'):
                        im_instructions_unencoded.append(instr.strip())
            else:
                other_query_parts.append(part)

    final_im_parts = []

    if not img_crop_disable:
        final_im_parts.append('Crop,rect=(0,0,800,450),gravity=Center')

    final_im_parts.extend(im_instructions_unencoded)

    final_im_parts.append(f'Resize,width={target_width}')

    im_value_for_url = ';'.join(final_im_parts)

    new_query_parts_for_url = []
    if im_value_for_url:
        new_query_parts_for_url.append(f"im={im_value_for_url}")

    new_query_parts_for_url.extend(other_query_parts)

    new_query_string = '&'.join(new_query_parts_for_url)

    return parsed_url._replace(query=new_query_string).geturl()


def generate_xmltv(data_list: List[Dict[str, Any]], output_file: str, xmltv_timezone: str = 'Europe/Berlin', img_crop_disable: bool = False) -> None:
    """
    Generates an XMLTV-compliant XML file from the scraped data using lxml.etree.

    Applies XML character sanitation to all string content.

    :param data_list: A list of dictionaries, each representing a program item.
                      Expected to have 'starttime_xmltv', 'endtime_xmltv', and 'rating_cleaned' fields.
    :type data_list: List[Dict[str, Any]]
    :param output_file: The path to the output XMLTV file.
    :type output_file: str
    :param xmltv_timezone: The timezone string to use for XMLTV header and program times.
    :type xmltv_timezone: str
    :param img_crop_disable: If True, disables default image cropping.
    :type img_crop_disable: bool
    :returns: None
    :rtype: None
    """
    logger.debug(f"Generating XMLTV file: {output_file} using lxml.etree.")

    channels: Dict[str, Dict[str, str]] = {}
    for item in data_list:
        sourceid: Optional[str] = item.get('sourceid')
        channelname: Optional[str] = item.get('channelname')
        if sourceid and sourceid not in channels and channelname:
            channels[sourceid] = {'name': channelname}

    if not data_list:
        logger.warning("No program data to generate XMLTV. Output file will not be created.")
        return

    try:
        target_tz = ZoneInfo(xmltv_timezone)
    except Exception as e:
        logger.error(f"Unknown timezone '{xmltv_timezone}' for XMLTV header. Falling back to UTC. Error: {e}")
        target_tz = timezone.utc

    now_in_target_tz = datetime.now(timezone.utc).astimezone(target_tz)
    tv_date_str = now_in_target_tz.strftime('%Y%m%d%H%M%S %z').replace(' ', '')

    tv_element = lxml.etree.Element(
        'tv',
        date=tv_date_str,
        **{
            'source-info-name': _sanitize_xml_string('TVSpielfilm.de') or '',
            'source-info-url': _sanitize_xml_string('https://m.tvspielfilm.de/') or '',
            'generator-info-name': _sanitize_xml_string('TVSpielfilm-Scraper/1.0') or ''
        }
    )

    for sourceid, channel_info in sorted(channels.items()):
        robust_channel_id: str = f"{sourceid.lower()}.tvs"
        channel_elem = lxml.etree.SubElement(tv_element, 'channel', id=robust_channel_id)
        display_name_elem = lxml.etree.SubElement(channel_elem, 'display-name')
        display_name_elem.text = channel_info['name']

    image_sizes_to_generate = {
        150: '1',
        300: '2',
        600: '3'
    }

    for item in data_list:
        start_time_xmltv: Optional[str] = item.get('starttime_xmltv')
        stop_time_xmltv: Optional[str] = item.get('endtime_xmltv')

        if not start_time_xmltv or not item.get('sourceid') or not item.get('title'):
            logger.warning(f"Skipping program due to missing essential data: {item.get('title', 'N/A')} on {item.get('channelname', 'N/A')}")
            continue

        robust_channel_id = f"{item['sourceid'].lower()}.tvs"
        programme_attrib: Dict[str, str] = {
            'start': start_time_xmltv,
            'channel': robust_channel_id
        }
        if stop_time_xmltv:
            programme_attrib['stop'] = stop_time_xmltv

        programme_elem = lxml.etree.SubElement(tv_element, 'programme', **programme_attrib)

        title_elem = lxml.etree.SubElement(programme_elem, 'title')
        title_elem.text = item['title']

        full_description_parts: List[str] = []
        if item.get('shortdescription'):
            full_description_parts.append(item['shortdescription'])
        if item.get('longdescription'):
            full_description_parts.append(item['longdescription'])
        if full_description_parts:
            desc_elem = lxml.etree.SubElement(programme_elem, 'desc')
            desc_elem.text = '\n'.join(full_description_parts)

        if item.get('director') or item.get('cast') or item.get('screenplay'):
            credits_elem = lxml.etree.SubElement(programme_elem, 'credits')
            if item.get('director'):
                director_elem = lxml.etree.SubElement(credits_elem, 'director')
                director_elem.text = item['director']

            if item.get('cast'):
                cast_entries: List[str] = [c.strip() for c in item['cast'].split(',') if c.strip()]
                for entry in cast_entries:
                    match: Optional[re.Match[str]] = re_cast_entry_match.match(entry)
                    if match:
                        actor_name: str = match.group(1).strip()
                        actor_role: str = match.group(2).strip()
                        actor_elem = lxml.etree.SubElement(credits_elem, 'actor', role=actor_role)
                        actor_elem.text = actor_name
                    else:
                        actor_elem = lxml.etree.SubElement(credits_elem, 'actor')
                        actor_elem.text = entry

            if item.get('screenplay'):
                writers: List[str] = [w.strip() for w in item['screenplay'].split(',') if w.strip()]
                for writer in writers:
                    writer_elem = lxml.etree.SubElement(credits_elem, 'writer')
                    writer_elem.text = writer

        if item.get('year'):
            date_elem = lxml.etree.SubElement(programme_elem, 'date')
            date_elem.text = str(item['year'])

        if item.get('genre'):
            categories: List[str] = [c.strip() for c in item['genre'].split(',') if c.strip()]
            for category in categories:
                category_elem = lxml.etree.SubElement(programme_elem, 'category')
                category_elem.text = category

        if item.get('keyword'):
            keyword_elem = lxml.etree.SubElement(programme_elem, 'keyword')
            keyword_elem.text = item['keyword']

        if item.get('duration') is not None:
            length_minutes: int = round(item['duration'] / 60)
            length_elem = lxml.etree.SubElement(programme_elem, 'length', units='minutes')
            length_elem.text = str(length_minutes)

        if item.get('link'):
            url_elem = lxml.etree.SubElement(programme_elem, 'url')
            url_elem.text = item['link']

        if item.get('country'):
            country_elem = lxml.etree.SubElement(programme_elem, 'country')
            country_elem.text = item['country']

        if item.get('parentalrating') is not None:
            rating_elem = lxml.etree.SubElement(programme_elem, 'rating', system='FSK')
            value_elem = lxml.etree.SubElement(rating_elem, 'value')
            value_elem.text = str(item['parentalrating'])

        if item.get('numrating') is not None:
            star_rating_elem = lxml.etree.SubElement(programme_elem, 'star-rating', system='TVSpielfilm')
            value_elem = lxml.etree.SubElement(star_rating_elem, 'value')
            value_elem.text = f"{item['numrating']} / 3"

        if item.get('imdbrating'):
            imdb_star_rating_elem = lxml.etree.SubElement(programme_elem, 'star-rating', system='IMDB')
            value_elem = lxml.etree.SubElement(imdb_star_rating_elem, 'value')
            value_elem.text = f"{item['imdbrating']} / 10"

        if item.get('txtrating'):
            review_elem = lxml.etree.SubElement(programme_elem, 'review', type='text', source='TVSpielfilm Editorial')
            review_elem.text = item['txtrating']

        if item.get('rating_cleaned'):
            review_detailed_elem = lxml.etree.SubElement(programme_elem, 'review', type='text', source='TVSpielfilm Detailed Rating')
            review_detailed_elem.text = item['rating_cleaned']

        if item.get('image_url'):
            for width, size_attr_value in image_sizes_to_generate.items():
                processed_image_url = _build_image_url_with_params(
                    item['image_url'],
                    width,
                    img_crop_disable
                )
                image_elem = lxml.etree.SubElement(programme_elem, 'image', type='still', size=size_attr_value, orient='L', system='tvsp')
                image_elem.text = processed_image_url
    
    tree = lxml.etree.ElementTree(tv_element)

    try:
        tree.write(output_file, pretty_print=True, encoding='utf-8', xml_declaration=True, doctype='<!DOCTYPE tv SYSTEM "xmltv.dtd">')
        logger.info(f"XMLTV file '{output_file}' successfully generated.")
    except Exception as e:
        logger.error(f"Error generating XMLTV file with lxml.etree: {e}")
        raise


if __name__ == '__main__':
    DEFAULT_OUTPUT_FILE: str = 'tvspielfilm'

    parser = argparse.ArgumentParser(
        description="""
A lean web scraper for TVSpielfilm.de to extract TV program data.
This script uses requests and lxml.html for HTML parsing,
supports caching with manual conditional fetching, parallel fetching,
and handles rate limiting with exponential backoff.
""",
        formatter_class=RawTextHelpFormatter,
        epilog="""
Examples:
  Scrape today's program for all channels and save as XMLTV:
    python3 tvs-scraper.py

  Scrape program for ARD and ZDF for the next 3 days as JSON:
    python3 tvs-scraper.py --channel-ids ARD,ZDF --days 3 --output-format json

  List all available channels:
    python3 tvs-scraper.py --list-channels

  Scrape program for a specific date with debug logging:
    python3 tvs-scraper.py --date 20250523 --log-verbose

  Clear the entire cache before scraping:
    python3 tvs-scraper.py --cache-clear

  Scrape program for today and output XMLTV with Europe/Berlin timezone:
    python3 tvs-scraper.py --xmltv-timezone Europe/Berlin
"""
    )

    parser.add_argument(
        '--list-channels',
        action='store_true',
        help='Lists all available channel IDs and their names, then exits.'
    )
    parser.add_argument(
        '--channel-ids',
        type=str,
        help='Comma-separated list of channel IDs (e.g., "ARD,ZDF"). If not specified, all channels will be scraped.'
    )
    parser.add_argument(
        '--channel-ids-file',
        type=str,
        help='Path to a file containing a comma-separated list of channel IDs. If specified, this option takes precedence over --channel-ids.'
    )

    parser.add_argument(
        '--date',
        type=str,
        help='Specific date to scrape inYYYYMMDD format (e.g., 20250523). If specified, only this date will be scraped and --days will be ignored.'
    )
    parser.add_argument(
        '--days',
        type=int,
        default=DEFAULT_DAYS,
        help=f'Number of days to scrape (1-{MAX_DAYS_TO_SCRAPE}). Default: {DEFAULT_DAYS} (means today only). Ignored if --date is specified.'
    )

    parser.add_argument(
        '--output-file',
        type=str,
        default=DEFAULT_OUTPUT_FILE,
        help=f'Path to the output file. If the default filename is used, the file extension will be automatically appended based on --output-format (e.g., .json for "json", .xml for "xmltv"). If a custom filename is provided, it will be used exactly as specified. Default: "{DEFAULT_OUTPUT_FILE}".'
    )
    parser.add_argument(
        '--output-format',
        type=str,
        default=DEFAULT_OUTPUT_FORMAT,
        choices=['xmltv', 'json'],
        help=f'Output format: "xmltv", "json" (JSON array). Default: {DEFAULT_OUTPUT_FORMAT}.'
    )
    parser.add_argument(
        '--xmltv-timezone',
        type=str,
        default='Europe/Berlin',
        help='Specifies the timezone to use for XMLTV output (e.g., "Europe/Berlin", "America/New_York"). This affects the timestamps in the XMLTV file. Default: "Europe/Berlin".'
    )
    parser.add_argument(
        '--img-size',
        type=str,
        choices=['150', '300', '600'],
        default=DEFAULT_IMG_SIZE,
        help=f'Image size to extract ("150", "300" or "600"). Default: {DEFAULT_IMG_SIZE}.'
    )
    parser.add_argument(
        '--img-check',
        action='store_true',
        dest='check_img',
        help='If set, an additional HEAD request will be performed to check the validity of image URLs. This may increase scraping time.'
    )
    parser.add_argument(
        '--img-crop-disable',
        action='store_true',
        dest='img_crop_disable',
        help='If set, disables the default 16:9 image cropping for images without existing crop instructions.'
    )

    parser.add_argument(
        '--log-level',
        type=str,
        default='WARNING',
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
        help='Sets the logging level. Options: DEBUG, INFO, WARNING, ERROR, CRITICAL. Default: WARNING.'
    )
    parser.add_argument(
        '--log-verbose',
        action='store_true',
        dest='verbose',
        help='Enables debug logging output (overrides --log-level to DEBUG).'
    )
    parser.add_argument(
        '--log-syslog',
        action='store_true',
        dest='use_syslog',
        help='Sends log output to Syslog (Linux Logger Utility).'
    )
    parser.add_argument(
        '--log-syslog-tag',
        type=str,
        default=DEFAULT_SYSLOG_TAG,
        dest='syslog_tag',
        help=f'Identifier (tag) for Syslog messages. Default: "{DEFAULT_SYSLOG_TAG}".'
    )

    parser.add_argument(
        '--cache-dir',
        type=str,
        help=f'Specifies a custom directory for cache files. Default is a subdirectory "{DEFAULT_CACHE_SUBDIR}" in the system temporary directory.'
    )
    parser.add_argument(
        '--cache-clear',
        action='store_true',
        dest='clear_cache',
        help='Clears the entire cache directory (including processed data cache) before scraping begins.'
    )
    parser.add_argument(
        '--cache-disable',
        action='store_true',
        default=DEFAULT_CACHE_DISABLE,
        dest='disable_cache',
        help='Disables caching of processed JSON data to disk. Default: False (cache is enabled).'
    )
    parser.add_argument(
        '--cache-ttl',
        type=int,
        default=DEFAULT_CACHE_TTL_SECONDS,
        help=f'Cache Time To Live in seconds. This defines how long a processed JSON file is considered "fresh" and used directly without re-scraping HTML. Default: {DEFAULT_CACHE_TTL_SECONDS // 3600} hours.'
    )
    parser.add_argument(
        '--cache-validation-tolerance',
        type=int,
        default=DEFAULT_CACHE_VALIDATION_TOLERANCE,
        help=f'Tolerance in bytes for content-length comparison when ETag/Last-Modified fails to return 304. Default: {DEFAULT_CACHE_VALIDATION_TOLERANCE} bytes.'
    )
    parser.add_argument(
        '--cache-keep',
        action='store_true',
        dest='keep_past_cache',
        help='If set, cache files for past days will NOT be automatically deleted. By default, past days\' cache files are deleted.'
    )

    parser.add_argument(
        '--max-workers',
        type=int,
        default=os.cpu_count() * 5,
        help=f'Maximum number of concurrent workers for data fetching. Default: {os.cpu_count() * 5}.'
    )
    parser.add_argument(
        '--max-retries',
        type=int,
        default=DEFAULT_MAX_RETRIES,
        help=f'Maximum number of retries for failed HTTP requests (e.g., 429, 5xx, connection errors). Default: {DEFAULT_MAX_RETRIES}.'
    )
    parser.add_argument(
        '--min-request-delay',
        type=float,
        default=DEFAULT_MIN_REQUEST_DELAY,
        help=f'Minimum delay in seconds between HTTP requests. Applies only to live fetches. Default: {DEFAULT_MIN_REQUEST_DELAY}s.'
    )
    parser.add_argument(
        '--max-schedule-retries',
        type=int,
        default=DEFAULT_MAX_SCHEDULE_RETRIES,
        help=f'Maximum number of retries for application-level errors during schedule parsing/generation. Default: {DEFAULT_MAX_SCHEDULE_RETRIES}.'
    )
    parser.add_argument(
        '--timeout-http',
        type=int,
        default=DEFAULT_HTTP_TIMEOUT,
        help=f'Timeout in seconds for all HTTP requests (GET, HEAD). Default: {DEFAULT_HTTP_TIMEOUT}s.'
    )

    args = parser.parse_args()

    if args.verbose:
        numeric_level = logging.DEBUG
        logger.info("Verbose mode enabled: Setting log level to DEBUG.")
    else:
        numeric_level = getattr(logging, args.log_level.upper(), None)
        if not isinstance(numeric_level, int):
            raise ValueError(f'Invalid log level: {args.log_level}')

    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)

    syslog_formatter = logging.Formatter(
        fmt='%(asctime)s ' + f'{args.syslog_tag}: %(message)s',
        datefmt="%b %d %H:%M:%S"
    )
    console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

    if args.use_syslog:
        try:
            syslog_handler = logging.handlers.SysLogHandler(address='/dev/log', facility=logging.handlers.SysLogHandler.LOG_USER)
            syslog_handler.setFormatter(syslog_formatter)
            logging.root.addHandler(syslog_handler)
            logger.info(f"Logging to syslog enabled with tag: '{args.syslog_tag}'.")
        except Exception as e:
            logger.error(f"Failed to set up syslog logging to '/dev/log'. This often means the syslog daemon is not running or '/dev/log' is not accessible: {e}. Falling back to console logging.")
            console_handler = logging.StreamHandler(sys.stderr)
            console_handler.setFormatter(console_formatter)
            logging.root.addHandler(console_handler)
    else:
        console_handler = logging.StreamHandler(sys.stderr)
        console_handler.setFormatter(console_formatter)
        logging.root.addHandler(console_handler)

    logging.root.setLevel(numeric_level)
    logger.setLevel(numeric_level)

    base_cache_path: str = args.cache_dir if args.cache_dir else os.path.join(tempfile.gettempdir(), DEFAULT_CACHE_SUBDIR)

    if args.clear_cache:
        if os.path.exists(base_cache_path):
            logger.info(f"Clearing cache directory: {base_cache_path}")
            shutil.rmtree(base_cache_path)
            logger.info("Cache cleared.")
        else:
            logger.info(f"No cache found at {base_cache_path} to clear.")

    scraper = TvsLeanScraper(
        channel_ids=args.channel_ids,
        days=args.days,
        img_size=args.img_size,
        check_img=args.check_img,
        start_date_str=args.date,
        cache_dir_path=args.cache_dir,
        max_workers=args.max_workers,
        max_retries=args.max_retries,
        channel_ids_file=getattr(args, 'channel_ids_file', None),
        min_request_delay=args.min_request_delay,
        max_schedule_retries=args.max_schedule_retries,
        disable_cache=args.disable_cache,
        cache_ttl=args.cache_ttl,
        keep_past_cache=args.keep_past_cache,
        cache_clear=args.clear_cache,
        xmltv_timezone=args.xmltv_timezone,
        cache_validation_tolerance=args.cache_validation_tolerance,
        img_crop_disable=args.img_crop_disable,
        http_timeout=args.timeout_http
    )

    if args.list_channels:
        logger.info("Fetching available channels...")
        channel_list: List[Dict[str, str]] = scraper._get_channel_list()
        if channel_list:
            print("\n--- Available Channels (ID: Name) ---")
            for channel in sorted(channel_list, key=lambda x: x['name'].lower()):
                print(f"{channel['source_id'].lower()}: {channel['name']}")
            print("-------------------------------------\n")
        else:
            logger.warning("No channels found.")
        sys.exit(0)

    start_time: float = time.time()
    try:
        extracted_data: List[Dict[str, Any]] = scraper.run_scraper()
    except KeyboardInterrupt:
        print("\nUser interruption (Ctrl-C)")
        sys.exit(1)
    end_time: float = time.time()

    elapsed_time: float = end_time - start_time

    hours: float
    minutes: float
    seconds: float
    hours, remainder = divmod(elapsed_time, 3600)
    minutes, seconds = divmod(remainder, 60)

    default_output_file_arg: str = parser.get_default('output_file')
    output_filename: str = args.output_file

    if args.output_format == 'json':
        if output_filename == default_output_file_arg and not output_filename.endswith('.json'):
            output_filename = f"{output_filename}.json"

        logger.info(f"Writing items to {output_filename} in JSON array format.")
        with open(output_filename, 'w', encoding='utf-8') as f:
            json.dump(extracted_data, f, ensure_ascii=False, indent=4)
        logger.info(f"Data successfully saved. Total {len(extracted_data)} items written.")
    elif args.output_format == 'xmltv':
        if output_filename == default_output_file_arg and not output_filename.endswith('.xml'):
            output_filename = f"{output_filename}.xml"

        generate_xmltv(extracted_data, output_filename, scraper.xmltv_timezone, scraper.img_crop_disable)

    else:
        logger.warning("No data extracted or invalid output format specified. Output file will not be created.")

    logger.info(f"Scraping and data processing completed in {int(hours)}h {int(minutes)}m {seconds:.2f}s.")
